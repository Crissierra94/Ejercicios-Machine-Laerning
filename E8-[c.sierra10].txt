Ventajas de usar ensemble en maching learning.
Los métodos de ensemble en machine learning son modelos que combinan diferentes técnicas de machone learning con el objetivo de construir un único modelo predictivo que produzca como resultado una varianza menor, sesgo menor y mejores predicciones. En general, el uso de varios modelos provee mejores resultados que un solo modelo. Entre los métodos de ensembling se encuentra bagging, boosting y stacking. 
Usar métodos de ensembling provee una serie de ventajas frente a los otros modelos tales como:  aumenta el poder predictivo del modelo, son técnicas de fácil implementación, es poco probable tener problemas de overfitting con este método y, adicionalmente, generan modelos más estables. 
En particular, con métodos de bagging, se busca reducir la varianza de la predicción mediante un aumento de los datos de entrenamiento usando combinaciones con repetición, lo que no aumenta el poder predictivo, pero sí reduce la varianza. Por su parte, boosting genera una serie de modelos sobre los datos con el fin de combinar los resultados con base en una función de costo particular. Finalmente, stacking busca ambos objetivos, reducir varianza y aumentar el poder predictivo del modelo, y utiliza como función de costos una regresión logística con la que combina todos los modelos. 
