{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 15\n",
    "\n",
    "# Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- Fraud Detection Dataset from Microsoft Azure: [data](http://gallery.cortanaintelligence.com/Experiment/8e9fe4e03b8b4c65b9ca947c72b8e463)\n",
    "\n",
    "Fraud detection is one of the earliest industrial applications of data mining and machine learning. Fraud detection is typically handled as a binary classification problem, but the class population is unbalanced because instances of fraud are usually very rare compared to the overall volume of transactions. Moreover, when fraudulent transactions are discovered, the business typically takes measures to block the accounts from transacting to prevent further losses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accountAge</th>\n",
       "      <th>digitalItemCount</th>\n",
       "      <th>sumPurchaseCount1Day</th>\n",
       "      <th>sumPurchaseAmount1Day</th>\n",
       "      <th>sumPurchaseAmount30Day</th>\n",
       "      <th>paymentBillingPostalCode - LogOddsForClass_0</th>\n",
       "      <th>accountPostalCode - LogOddsForClass_0</th>\n",
       "      <th>paymentBillingState - LogOddsForClass_0</th>\n",
       "      <th>accountState - LogOddsForClass_0</th>\n",
       "      <th>paymentInstrumentAgeInAccount</th>\n",
       "      <th>ipState - LogOddsForClass_0</th>\n",
       "      <th>transactionAmount</th>\n",
       "      <th>transactionAmountUSD</th>\n",
       "      <th>ipPostalCode - LogOddsForClass_0</th>\n",
       "      <th>localHour - LogOddsForClass_0</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>720.25</td>\n",
       "      <td>5.064533</td>\n",
       "      <td>0.421214</td>\n",
       "      <td>1.312186</td>\n",
       "      <td>0.566395</td>\n",
       "      <td>3279.574306</td>\n",
       "      <td>1.218157</td>\n",
       "      <td>599.00</td>\n",
       "      <td>626.164650</td>\n",
       "      <td>1.259543</td>\n",
       "      <td>4.745402</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1185.44</td>\n",
       "      <td>2530.37</td>\n",
       "      <td>0.538996</td>\n",
       "      <td>0.481838</td>\n",
       "      <td>4.401370</td>\n",
       "      <td>4.500157</td>\n",
       "      <td>61.970139</td>\n",
       "      <td>4.035601</td>\n",
       "      <td>1185.44</td>\n",
       "      <td>1185.440000</td>\n",
       "      <td>3.981118</td>\n",
       "      <td>4.921349</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.064533</td>\n",
       "      <td>5.096396</td>\n",
       "      <td>3.056357</td>\n",
       "      <td>3.155226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.314186</td>\n",
       "      <td>32.09</td>\n",
       "      <td>32.090000</td>\n",
       "      <td>5.008490</td>\n",
       "      <td>4.742303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.064533</td>\n",
       "      <td>5.096396</td>\n",
       "      <td>3.331154</td>\n",
       "      <td>3.331239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.529398</td>\n",
       "      <td>133.28</td>\n",
       "      <td>132.729554</td>\n",
       "      <td>1.324925</td>\n",
       "      <td>4.745402</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>132.73</td>\n",
       "      <td>5.412885</td>\n",
       "      <td>0.342945</td>\n",
       "      <td>5.563677</td>\n",
       "      <td>4.086965</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>3.529398</td>\n",
       "      <td>543.66</td>\n",
       "      <td>543.660000</td>\n",
       "      <td>2.693451</td>\n",
       "      <td>4.876771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accountAge  digitalItemCount  sumPurchaseCount1Day  sumPurchaseAmount1Day  \\\n",
       "0        2000                 0                     0                   0.00   \n",
       "1          62                 1                     1                1185.44   \n",
       "2        2000                 0                     0                   0.00   \n",
       "3           1                 1                     0                   0.00   \n",
       "4           1                 1                     0                   0.00   \n",
       "\n",
       "   sumPurchaseAmount30Day  paymentBillingPostalCode - LogOddsForClass_0  \\\n",
       "0                  720.25                                      5.064533   \n",
       "1                 2530.37                                      0.538996   \n",
       "2                    0.00                                      5.064533   \n",
       "3                    0.00                                      5.064533   \n",
       "4                  132.73                                      5.412885   \n",
       "\n",
       "   accountPostalCode - LogOddsForClass_0  \\\n",
       "0                               0.421214   \n",
       "1                               0.481838   \n",
       "2                               5.096396   \n",
       "3                               5.096396   \n",
       "4                               0.342945   \n",
       "\n",
       "   paymentBillingState - LogOddsForClass_0  accountState - LogOddsForClass_0  \\\n",
       "0                                 1.312186                          0.566395   \n",
       "1                                 4.401370                          4.500157   \n",
       "2                                 3.056357                          3.155226   \n",
       "3                                 3.331154                          3.331239   \n",
       "4                                 5.563677                          4.086965   \n",
       "\n",
       "   paymentInstrumentAgeInAccount  ipState - LogOddsForClass_0  \\\n",
       "0                    3279.574306                     1.218157   \n",
       "1                      61.970139                     4.035601   \n",
       "2                       0.000000                     3.314186   \n",
       "3                       0.000000                     3.529398   \n",
       "4                       0.001389                     3.529398   \n",
       "\n",
       "   transactionAmount  transactionAmountUSD  ipPostalCode - LogOddsForClass_0  \\\n",
       "0             599.00            626.164650                          1.259543   \n",
       "1            1185.44           1185.440000                          3.981118   \n",
       "2              32.09             32.090000                          5.008490   \n",
       "3             133.28            132.729554                          1.324925   \n",
       "4             543.66            543.660000                          2.693451   \n",
       "\n",
       "   localHour - LogOddsForClass_0  Label  \n",
       "0                       4.745402      0  \n",
       "1                       4.921349      0  \n",
       "2                       4.742303      0  \n",
       "3                       4.745402      0  \n",
       "4                       4.876771      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/albahnsen/PracticalMachineLearningClass/master/datasets/15_fraud_detection.csv.zip'\n",
    "df = pd.read_csv(url, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accountAge</th>\n",
       "      <th>digitalItemCount</th>\n",
       "      <th>sumPurchaseCount1Day</th>\n",
       "      <th>sumPurchaseAmount1Day</th>\n",
       "      <th>sumPurchaseAmount30Day</th>\n",
       "      <th>paymentBillingPostalCode - LogOddsForClass_0</th>\n",
       "      <th>accountPostalCode - LogOddsForClass_0</th>\n",
       "      <th>paymentBillingState - LogOddsForClass_0</th>\n",
       "      <th>accountState - LogOddsForClass_0</th>\n",
       "      <th>paymentInstrumentAgeInAccount</th>\n",
       "      <th>ipState - LogOddsForClass_0</th>\n",
       "      <th>transactionAmount</th>\n",
       "      <th>transactionAmountUSD</th>\n",
       "      <th>ipPostalCode - LogOddsForClass_0</th>\n",
       "      <th>localHour - LogOddsForClass_0</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>720.25</td>\n",
       "      <td>5.064533</td>\n",
       "      <td>0.421214</td>\n",
       "      <td>1.312186</td>\n",
       "      <td>0.566395</td>\n",
       "      <td>3279.574306</td>\n",
       "      <td>1.218157</td>\n",
       "      <td>599.00</td>\n",
       "      <td>626.164650</td>\n",
       "      <td>1.259543</td>\n",
       "      <td>4.745402</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1185.44</td>\n",
       "      <td>2530.37</td>\n",
       "      <td>0.538996</td>\n",
       "      <td>0.481838</td>\n",
       "      <td>4.401370</td>\n",
       "      <td>4.500157</td>\n",
       "      <td>61.970139</td>\n",
       "      <td>4.035601</td>\n",
       "      <td>1185.44</td>\n",
       "      <td>1185.440000</td>\n",
       "      <td>3.981118</td>\n",
       "      <td>4.921349</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.064533</td>\n",
       "      <td>5.096396</td>\n",
       "      <td>3.056357</td>\n",
       "      <td>3.155226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.314186</td>\n",
       "      <td>32.09</td>\n",
       "      <td>32.090000</td>\n",
       "      <td>5.008490</td>\n",
       "      <td>4.742303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.064533</td>\n",
       "      <td>5.096396</td>\n",
       "      <td>3.331154</td>\n",
       "      <td>3.331239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.529398</td>\n",
       "      <td>133.28</td>\n",
       "      <td>132.729554</td>\n",
       "      <td>1.324925</td>\n",
       "      <td>4.745402</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>132.73</td>\n",
       "      <td>5.412885</td>\n",
       "      <td>0.342945</td>\n",
       "      <td>5.563677</td>\n",
       "      <td>4.086965</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>3.529398</td>\n",
       "      <td>543.66</td>\n",
       "      <td>543.660000</td>\n",
       "      <td>2.693451</td>\n",
       "      <td>4.876771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accountAge  digitalItemCount  sumPurchaseCount1Day  sumPurchaseAmount1Day  \\\n",
       "0        2000                 0                     0                   0.00   \n",
       "1          62                 1                     1                1185.44   \n",
       "2        2000                 0                     0                   0.00   \n",
       "3           1                 1                     0                   0.00   \n",
       "4           1                 1                     0                   0.00   \n",
       "\n",
       "   sumPurchaseAmount30Day  paymentBillingPostalCode - LogOddsForClass_0  \\\n",
       "0                  720.25                                      5.064533   \n",
       "1                 2530.37                                      0.538996   \n",
       "2                    0.00                                      5.064533   \n",
       "3                    0.00                                      5.064533   \n",
       "4                  132.73                                      5.412885   \n",
       "\n",
       "   accountPostalCode - LogOddsForClass_0  \\\n",
       "0                               0.421214   \n",
       "1                               0.481838   \n",
       "2                               5.096396   \n",
       "3                               5.096396   \n",
       "4                               0.342945   \n",
       "\n",
       "   paymentBillingState - LogOddsForClass_0  accountState - LogOddsForClass_0  \\\n",
       "0                                 1.312186                          0.566395   \n",
       "1                                 4.401370                          4.500157   \n",
       "2                                 3.056357                          3.155226   \n",
       "3                                 3.331154                          3.331239   \n",
       "4                                 5.563677                          4.086965   \n",
       "\n",
       "   paymentInstrumentAgeInAccount  ipState - LogOddsForClass_0  \\\n",
       "0                    3279.574306                     1.218157   \n",
       "1                      61.970139                     4.035601   \n",
       "2                       0.000000                     3.314186   \n",
       "3                       0.000000                     3.529398   \n",
       "4                       0.001389                     3.529398   \n",
       "\n",
       "   transactionAmount  transactionAmountUSD  ipPostalCode - LogOddsForClass_0  \\\n",
       "0             599.00            626.164650                          1.259543   \n",
       "1            1185.44           1185.440000                          3.981118   \n",
       "2              32.09             32.090000                          5.008490   \n",
       "3             133.28            132.729554                          1.324925   \n",
       "4             543.66            543.660000                          2.693451   \n",
       "\n",
       "   localHour - LogOddsForClass_0  Label  \n",
       "0                       4.745402      0  \n",
       "1                       4.921349      0  \n",
       "2                       4.742303      0  \n",
       "3                       4.745402      0  \n",
       "4                       4.876771      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((138721, 16), 797, 0.0057453449730033666)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df.Label.sum(), df.Label.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 15.1\n",
    "\n",
    "Estimate a Logistic Regression and a Decision Tree\n",
    "\n",
    "Evaluate using the following metrics:\n",
    "* Accuracy\n",
    "* F1-Score\n",
    "* F_Beta-Score (Beta=10)\n",
    "\n",
    "Comment about the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Label'], axis = 1)\n",
    "y = df['Label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9942043193679536 F1 score 0.0 F Beta score 0.0\n"
     ]
    }
   ],
   "source": [
    "performance_scores = []\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Accuracy',metrics.accuracy_score(y_test, y_pred),'F1 score',metrics.f1_score(y_test, y_pred),'F Beta score',metrics.fbeta_score(y_test, y_pred, beta = 10))\n",
    "performance_scores.append(['Unbalanced','Logistic Regression','No target','No k',metrics.accuracy_score(y_pred, y_test), metrics.f1_score(y_pred, y_test), metrics.fbeta_score(y_pred, y_test,10)])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9942331535999539 F1 score 0.06542056074766354 F Beta score 0.0356782398062172\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth = 4, random_state = 42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy',metrics.accuracy_score(y_test, y_pred),'F1 score',metrics.f1_score(y_test, y_pred),'F Beta score',metrics.fbeta_score(y_test, y_pred, beta = 10))\n",
    "performance_scores.append(['Unbalanced','Decision Tree','No target','No k',metrics.accuracy_score(y_pred, y_test), metrics.f1_score(y_pred, y_test), metrics.fbeta_score(y_pred, y_test,10)])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9942331535999539 F1 score 0.02912621359223301 F Beta score 0.015296849757673666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth = 4, random_state = 42, n_estimators=50)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy',metrics.accuracy_score(y_test, y_pred),'F1 score',metrics.f1_score(y_test, y_pred),'F Beta score',metrics.fbeta_score(y_test, y_pred, beta = 10))\n",
    "performance_scores.append(['Unbalanced','Random Forest','No target','No k',metrics.accuracy_score(y_pred, y_test), metrics.f1_score(y_pred, y_test), metrics.fbeta_score(y_pred, y_test,10)])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los 3 casos se observa que la precisión es la misma lo que lleva a pensar que es probable que con las tres alternativas se esté clasificando todos los datos en la categoría negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 15.2\n",
    "\n",
    "Under-sample the negative class using random-under-sampling\n",
    "\n",
    "Which is parameter for target_percentage did you choose?\n",
    "How the results change?\n",
    "\n",
    "**Only apply under-sampling to the training set, evaluate using the whole test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005778544705779994"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = y.shape[0]\n",
    "n_samples_0 = (y == 0).sum()\n",
    "n_samples_1 = (y == 1).sum()\n",
    "n_samples_0_new =  n_samples_1 / 0.5 - n_samples_1\n",
    "n_samples_0_new\n",
    "n_samples_0_new_per = n_samples_0_new / n_samples_0\n",
    "n_samples_0_new_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "757"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_ = y == 0\n",
    "np.random.seed(42)\n",
    "rand_1 = np.random.binomial(n=1, p=n_samples_0_new_per, size=n_samples)\n",
    "\n",
    "filter_ = filter_ & rand_1\n",
    "filter_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ = filter_ | (y == 1)\n",
    "filter_ = filter_.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12ab8f6bdd8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAJPCAYAAADBrYi9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3X90XeV95/v39xzpSLLkH7KxsbFNII4zCZipbETwXRMIJQMm0GnITVkLbpOYDIGhLbntrLlt03baJE3vKp3ODJ38mLTJNInJDKU0nSTcrhTHQ0Lg3pIEASYYSLAhgG1sZCxZWD+PzjnP/eNsG9nItiTLaNt+v9Y60dF3P3vvZyskfNbz7GfvSCkhSZKk/CrMdAckSZJ0dAY2SZKknDOwSZIk5ZyBTZIkKecMbJIkSTlnYJMkSco5A5skSVLOGdgkSZJyzsAmSZKUcw0z3YHpdsYZZ6RzzjlnprshSZJ0TI8++uirKaWFx2p3ygW2c845h66urpnuhiRJ0jFFxIsTaeeUqCRJUs4Z2CRJknLOwCZJkpRzp9w9bJIk6cQYHR1lx44dDA8Pz3RXTjrNzc0sW7aMxsbGKe1vYJMkSROyY8cOZs+ezTnnnENEzHR3ThopJfbu3cuOHTs499xzp3QMp0QlSdKEDA8Ps2DBAsPaJEUECxYsOK6RSQObJEmaMMPa1Bzv383AJkmSlHMGNkmSdMIMliu82j/CYLkyLcfbvXs3119/PStWrOC8887j6quv5tlnn+WFF15g1apV03KOIxkYGOCKK64A4N3vfjeVyuvXtGHDBlauXMnKlSvZsGHDtJ/bRQeSJGna9QyU2bhlN49v7yUBAaxe3s66VYuZ31qa0jFTSnzgAx9g/fr13H333QBs3ryZV155heXLl09f54/g4YcfZu3atfT29tLa2kpDQz1G9fT08OlPf5quri4iggsvvJBf/uVfpr29fdrO7QibJEmaVj0DZT53/1Ye397LotnNnDW3hUWzm3l8ey+f/95WegbKUzru97//fRobG7n11lsP1jo6OrjkkksOaffCCy9wySWXsGbNGtasWcM//dM/AbBr1y4uvfRSOjo6WLVqFQ899BDVapUbb7yRVatWccEFF3DHHXe84bzPPfccHR0dfOhDH+Kuu+7iwgsv5IknnqCjo4Pu7m42btzIFVdcwfz582lvb+eKK67gvvvum9I1HokjbJIkaVpt3LKbgXKFJXNbDtaKhWDJ3BZ29Q2xcctubrj47Ekfd8uWLVx44YXHbLdo0SI2bdpEc3MzW7du5YYbbqCrq4u77rqLdevW8Qd/8AdUq1UGBwfZvHkzO3fuZMuWLQDs27fvDcdbsWIFmzdv5pprruHOO+/kc5/7HBdddBHXXHMNADt37jxkhG/ZsmXs3Llz0td3NI6wSZKkaTNYrhwcWRvPgZG2oXL1hPVhdHSUm2++mQsuuIDrrruOp59+GoCLLrqIr371q3zqU5/iySefZPbs2bz1rW/l+eef5+Mf/zj33Xcfc+bMOeJxu7u7WbBgAU8++SQdHR0H6ymlN7Sd7tW0BjZJkjRtBstVEvURtfHU68HAFBYhnH/++Tz66KPHbHfHHXdw5pln8sQTT9DV1UW5XJ+CvfTSS3nwwQdZunQpH/7wh7nzzjtpb2/niSee4LLLLuMLX/gCH/vYx95wvFtvvZVVq1axdetWOjo6uO+++7jmmmsOTp8uW7aM7du3H2y/Y8cOzjrrrElf39EY2CRJ0rSZVSoSQLX2xlEnDtYTraXJ35V1+eWXMzIywpe//OWDtUceeYQf/OAHh7Tr6+tjyZIlFAoFvv71r1Ot1kfzXnzxRRYtWsTNN9/MTTfdxGOPPcarr75KrVbjgx/8IJ/5zGd47LHH3nDev/zLv+STn/wkf/iHf8i3vvUtrrnmGjZv3sy//bf/FoB169bx3e9+l97eXnp7e/nud7/LunXrJn19R+M9bJIkadrMKjWwenk7j2/vPeQetgO69w+zenk7LaXipI8dEXzzm9/kt37rt7j99ttpbm7mnHPO4S/+4i8Oaffrv/7rfPCDH+Tv/u7v+MVf/EVaW1sBeOCBB/jzP/9zGhsbaWtr484772Tnzp189KMfpVarAfCnf/qn4577Bz/4AR/5yEd46KGHeM973nPItvnz5/OHf/iHXHTRRQD80R/9EfPnz5/09R312sebdz2ZdXZ2pq6urpnuhiRJp5xnnnmGd77zncds1zNQ5vPf20r/SIVFs5spFoJqLdG9f5i2pgZuu3zllB/tcTIb7+8XEY+mlDqPta8jbJIkaVrNby1x2+UrDz6Hrf4UtsSas9u58vypP4ftdGZgmyaD5QqD5SqzSkVmTWFeXpKkU8n81hI3XHw2165eykC5QmupYUrToKozWRynE/EkZ0mSThUtpaJBbRoY2I7DgSc5D5QPnaN/fHsvW7v3n7Zz9JIkaXr5WI/jMPZJzgeeN3PgSc79IxU2btk9wz2UJEmnAgPbFOXhSc6SJOn0YGCbohP5JGdJkk4Z5QHo31P/OQ12797N9ddfz4oVKzjvvPO4+uqrefbZZ3nhhRdYtWrVtJzjSAYGBrjiiisAePe7302l8vq/46+66irmzZvHL/3SL52Qc3sP2xSNfZLzeKHteJ7kLEnSSW9gL/z0H2DHjzm4Km/Zu+AdvwStC6Z0yJQSH/jAB1i/fj133303AJs3b+aVV1455OXrJ8rDDz/M2rVr6e3tpbW1lYaG1/8d/9u//dsMDg7yV3/1Vyfk3I6wTdGBJzl37x8ed/vxPMlZkqST2sBeePA/wI5HoG0JzF1W/7njEXjwz+vbp+D73/8+jY2N3HrrrQdrHR0dXHLJJYe0e+GFF7jkkktYs2YNa9as4Z/+6Z8A2LVrF5deeikdHR2sWrWKhx56iGq1yo033siqVau44IILDr4fdKznnnuOjo4OPvShD3HXXXdx4YUX8sQTT9DR0UF3dzcA733ve5k9e/aUrmsiHP45DutWLWZr93529Q2N+yTndasWz3QXJUl68/30H+pToHOWvl4rFOu/v7azvv3C9ZM+7JYtW7jwwguP2W7RokVs2rSJ5uZmtm7dyg033EBXVxd33XUX69at4w/+4A+oVqsMDg6yefNmdu7cyZYtWwDYt2/fG463YsUKNm/ezDXXXMOdd97J5z73OS666CKuueaaSV/DVDnCdhwOPMn5wEjbrr5huvcPs+bsdh/pIUk6PZUH6tOgbUcYtGhbXN9eHjxhXRgdHeXmm2/mggsu4LrrruPpp58G4KKLLuKrX/0qn/rUp3jyySeZPXs2b33rW3n++ef5+Mc/zn333cecOXOOeNzu7m4WLFjAk08+SUdHxwnr/3gMbMfpwJOcP/3Lq/idq/4Zn/7lVVz/rrMNa5Kk01N5sH7PWuEItwQdqE9hEcL555/Po48+esx2d9xxB2eeeSZPPPEEXV1dlMtlAC699FIefPBBli5dyoc//GHuvPNO2tvbeeKJJ7jsssv4whe+wMc+9rE3HO/WW29l1apVbN26lY6ODu677z6uueaacadPTxSnRKeJT3KWJAkozaovMKhVxw9ttexxV6XWSR/68ssv5/d///f58pe/zM033wzAI488wuDgIG95y1sOtuvr62PZsmUUCgU2bNhAtVo/54svvsjSpUu5+eabGRgY4LHHHuPqq6+mVCrxwQ9+kBUrVnDjjTe+4bx/+Zd/yd/93d/x0ksv8cEPfpDf+Z3f4Z577pl0/4+HI2ySJGn6lFrrq0H7j/Dw+P7d9e2lWZM+dETwzW9+k02bNrFixQrOP/98PvWpT3HWWWcd0u7Xf/3X2bBhA2vXruXZZ5+ltbUeDh944AE6OjpYvXo1f//3f89v/uZvsnPnTi677DI6Ojq48cYb+dM//dNxz/2DH/yASy65hIceeoj3vOc9b9h+ySWXcN1113H//fezbNkyNm7cOOnrO+q1p5Sm9YAzrbOzM3V1dc10NyRJOuU888wzvPOd7zx2w4G99dWg5f76PWuFYn1krX83lNrg0t+e8qM9Tmbj/f0i4tGUUuex9j3mCFtENEfEjyPiiYh4KiI+ndW/FhE/j4jN2acjq0dEfDYitkXETyJizZhjrY+Irdln/Zj6hRHxZLbPZyMisvr8iNiUtd8UEe0T/qtIkqSZ0bqgHsqWXQT9u+C1HfWfy9512oa14zWRe9hGgMtTSv0R0Qj8vxHxj9m2304pfeOw9u8DVmafi4EvAhdHxHzgk0An9dsRH42Ie1NKvVmbW4AfAt8BrgL+EfgEcH9K6faI+ET2++9O/XIlSdKbonVB/dEdF1xXX2BQap3SNKjqjjnClur6s18bs8/R5lHfD9yZ7fdDYF5ELAHWAZtSSj1ZSNsEXJVtm5NSejjV52fvBK4dc6wN2fcNY+qSJGkGTPpWqtIsaFt42oe1470FbUKLDiKiGBGbgW7qoetH2ab/O5v2vCMimrLaUmD7mN13ZLWj1XeMUwc4M6W0CyD7uegI/bslIroiomvPnj0TuSRJkjRJzc3N7N2797jDx+kmpcTevXtpbm6e8jEm9FiPlFIV6IiIecA3I2IV8HvAbqAEfIn6VOUfU1/M+4ZDTKE+YSmlL2V9oLOz03+KJEk6AZYtW8aOHTtwcGTympubWbZs2ZT3n9Rz2FJK+yLiAeCqlNJ/zMojEfFV4P/Kft8BjH0D6zLg5ax+2WH1B7L6snHaA7wSEUtSSruyqdPuyfRXkiRNn8bGRs4999yZ7sZpaSKrRBdmI2tERAvwL4GfZgGKbEXntcCWbJd7gY9kq0XXAn3ZdOZG4MqIaM9We14JbMy27Y+ItdmxPgJ8e8yxDqwmXT+mLkmSdNqYyAjbEmBDRBSpB7x7Ukr/EBHfi4iF1Kc0NwO3Zu2/A1wNbAMGgY8CpJR6IuIzwCNZuz9OKfVk338N+BrQQn116IFVqLcD90TETcBLwHVTvVBJkqSTlQ/OlSRJmiHT9uBcSZIkzSwDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLumIEtIpoj4scR8UREPBURn87q50bEjyJia0T8bUSUsnpT9vu2bPs5Y471e1n9ZxGxbkz9qqy2LSI+MaY+7jkkSZJOJxMZYRsBLk8p/QLQAVwVEWuBPwPuSCmtBHqBm7L2NwG9KaW3AXdk7YiI84DrgfOBq4D/GhHFiCgCXwDeB5wH3JC15SjnkCRJOm0cM7Cluv7s18bsk4DLgW9k9Q3Atdn392e/k21/b0REVr87pTSSUvo5sA14V/bZllJ6PqVUBu4G3p/tc6RzSJIknTYmdA9bNhK2GegGNgHPAftSSpWsyQ5gafZ9KbAdINveBywYWz9snyPVFxzlHIf375aI6IqIrj179kzkkiRJkk4aEwpsKaVqSqkDWEZ9ROyd4zXLfsYRtk1Xfbz+fSml1JlS6ly4cOF4TSRJkk5ak1olmlLaBzwArAXmRURDtmkZ8HL2fQewHCDbPhfoGVs/bJ8j1V89yjkkSZJOGxNZJbowIuZl31uAfwk8A3wf+JWs2Xrg29n3e7PfybZ/L6WUsvr12SrSc4GVwI+BR4CV2YrQEvWFCfdm+xzpHJIkSaeNhmM3YQmwIVvNWQDuSSn9Q0Q8DdwdEX8CPA78ddb+r4GvR8Q26iNr1wOklJ6KiHuAp4EK8BsppSpARNwGbASKwFdSSk9lx/rdI5xDkiTptBH1gaxTR2dnZ+rq6prpbkiSJB1TRDyaUuo8VjvfdCBJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTl3DEDW0Qsj4jvR8QzEfFURPxmVv9UROyMiM3Z5+ox+/xeRGyLiJ9FxLox9auy2raI+MSY+rkR8aOI2BoRfxsRpazelP2+Ldt+znRevCRJ0slgIiNsFeDfpZTeCawFfiMizsu23ZFS6sg+3wHItl0PnA9cBfzXiChGRBH4AvA+4DzghjHH+bPsWCuBXuCmrH4T0JtSehtwR9ZOkiTptHLMwJZS2pVSeiz7vh94Blh6lF3eD9ydUhpJKf0c2Aa8K/tsSyk9n1IqA3cD74+IAC4HvpHtvwG4dsyxNmTfvwG8N2svSZJ02pjUPWzZlORq4EdZ6baI+ElEfCUi2rPaUmD7mN12ZLUj1RcA+1JKlcPqhxwr296XtZckSTptTDiwRUQb8PfAb6WUXgO+CKwAOoBdwH860HSc3dMU6kc71uF9uyUiuiKia8+ePUe9DkmSpJPNhAJbRDRSD2v/I6X0PwFSSq+klKoppRrwZepTnlAfIVs+ZvdlwMtHqb8KzIuIhsPqhxwr2z4X6Dm8fymlL6WUOlNKnQsXLpzIJUmSJJ00JrJKNIC/Bp5JKf3nMfUlY5p9ANiSfb8XuD5b4XkusBL4MfAIsDJbEVqivjDh3pRSAr4P/Eq2/3rg22OOtT77/ivA97L2kiRJp42GYzfhXwAfBp6MiM1Z7fepr/LsoD5F+QLwbwBSSk9FxD3A09RXmP5GSqkKEBG3ARuBIvCVlNJT2fF+F7g7Iv4EeJx6QCT7+fWI2EZ9ZO3647hWSZKkk1KcagNWnZ2dqaura6a7IUmSdEwR8WhKqfNY7XzTgSRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzBjZJkqScM7BJkiTlnIFNkiQp5wxskiRJOWdgkyRJyjkDmyRJUs4Z2CRJknLOwCZJkpRzxwxsEbE8Ir4fEc9ExFMR8ZtZfX5EbIqIrdnP9qweEfHZiNgWET+JiDVjjrU+a781ItaPqV8YEU9m+3w2IuJo55AkSTqdTGSErQL8u5TSO4G1wG9ExHnAJ4D7U0orgfuz3wHeB6zMPrcAX4R6+AI+CVwMvAv45JgA9sWs7YH9rsrqRzqHJEnSaeOYgS2ltCul9Fj2fT/wDLAUeD+wIWu2Abg2+/5+4M5U90NgXkQsAdYBm1JKPSmlXmATcFW2bU5K6eGUUgLuPOxY451DkiTptDGpe9gi4hxgNfAj4MyU0i6ohzpgUdZsKbB9zG47strR6jvGqXOUc0iSJJ02JhzYIqIN+Hvgt1JKrx2t6Ti1NIX6hEXELRHRFRFde/bsmcyukiRJuTehwBYRjdTD2v9IKf3PrPxKNp1J9rM7q+8Alo/ZfRnw8jHqy8apH+0ch0gpfSml1JlS6ly4cOFELkmSJOmkMZFVogH8NfBMSuk/j9l0L3Bgped64Ntj6h/JVouuBfqy6cyNwJUR0Z4tNrgS2Jht2x8Ra7NzfeSwY413DkmSpNNGwwTa/Avgw8CTEbE5q/0+cDtwT0TcBLwEXJdt+w5wNbANGAQ+CpBS6omIzwCPZO3+OKXUk33/NeBrQAvwj9mHo5xDkiTptBH1hZmnjs7OztTV1TXT3ZAkSTqmiHg0pdR5rHa+6UCSJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzh0zsEXEVyKiOyK2jKl9KiJ2RsTm7HP1mG2/FxHbIuJnEbFuTP2qrLYtIj4xpn5uRPwoIrZGxN9GRCmrN2W/b8u2nzNdFy1JknQymcgI29eAq8ap35FS6sg+3wGIiPOA64Hzs33+a0QUI6IIfAF4H3AecEPWFuDPsmOtBHqBm7L6TUBvSultwB1ZO0mSpNPOMQNbSulBoGeCx3s/cHdKaSSl9HNgG/Cu7LMtpfR8SqkM3A28PyICuBz4Rrb/BuDaMcfakH3/BvDerL0kSdJp5XjuYbstIn6STZm2Z7WlwPYxbXZktSPVFwD7UkqVw+qHHCvb3pe1f4OIuCUiuiKia8+ePcdxSZIkSfkz1cD2RWAF0AHsAv5TVh9vBCxNoX60Y72xmNKXUkqdKaXOhQsXHq3fkiRJJ50pBbaU0isppWpKqQZ8mfqUJ9RHyJaPaboMePko9VeBeRHRcFj9kGNl2+cy8alZSZKkU8aUAltELBnz6weAAytI7wWuz1Z4ngusBH4MPAKszFaElqgvTLg3pZSA7wO/ku2/Hvj2mGOtz77/CvC9rL0kSdJppeFYDSLib4DLgDMiYgfwSeCyiOigPkX5AvBvAFJKT0XEPcDTQAX4jZRSNTvObcBGoAh8JaX0VHaK3wXujog/AR4H/jqr/zXw9YjYRn1k7frjvlpJkqSTUJxqg1adnZ2pq6trprshSZJ0TBHxaEqp81jtfNOBJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHPHDGwR8ZWI6I6ILWNq8yNiU0RszX62Z/WIiM9GxLaI+ElErBmzz/qs/daIWD+mfmFEPJnt89mIiKOdQ5Ik6XQzkRG2rwFXHVb7BHB/SmklcH/2O8D7gJXZ5xbgi1APX8AngYuBdwGfHBPAvpi1PbDfVcc4hyRJ0mnlmIEtpfQg0HNY+f3Ahuz7BuDaMfU7U90PgXkRsQRYB2xKKfWklHqBTcBV2bY5KaWHU0oJuPOwY413DkmSpNPKVO9hOzOltAsg+7koqy8Fto9ptyOrHa2+Y5z60c7xBhFxS0R0RUTXnj17pnhJkiRJ+TTdiw5inFqaQn1SUkpfSil1ppQ6Fy5cONndJUmScm2qge2VbDqT7Gd3Vt8BLB/Tbhnw8jHqy8apH+0ckiRJp5WpBrZ7gQMrPdcD3x5T/0i2WnQt0JdNZ24EroyI9myxwZXAxmzb/ohYm60O/chhxxrvHJIkSaeVhmM1iIi/AS4DzoiIHdRXe94O3BMRNwEvAddlzb8DXA1sAwaBjwKklHoi4jPAI1m7P04pHVjI8GvUV6K2AP+YfTjKOSRJkk4rUV+ceero7OxMXV1dM90NSZKkY4qIR1NKncdq55sOJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJEk0C95fAAAgAElEQVRSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScs7AJkmSlHMGNkmSpJwzsEmSJOWcgU2SJCnnDGySJEk5Z2CTJEnKOQObJElSzhnYJEmScq5hpjtwKhgsVxgsV5lVKjKr5J9UkiRNL9PFcegZKLNxy24e395LAgJYvbyddasWM7+1NNPdkyRJpwgD2xT1DJT53P1bGShXWDS7mWIhqNYSj2/vZWv3fm67fKWhTZIkTQvvYZuijVt2M1CusGRuC8VCAFAsBEvmttA/UmHjlt0z3ENJknSqMLBNwWC5wuPbe1k0u3nc7YtmN/P49l6GytU3uWeSJOlUZGCbgsFylQQHR9YOV68HA+XKm9ovSZJ0ajKwTcGsUpEAqrU07vZ6PdHqilFJkjQNDGxTMKvUwOrl7XTvHx53e/f+YVYvb6elVHyTeyZJkk5FBrYpWrdqMW1NDezqGzo40latJXb1DdHW1MC6VYtnuIeSJOlU4ZzdFM1vLXHb5SsPPoet/hS2xJqz27nyfJ/DJkmSpo+B7TjMby1xw8Vnc+3qpQyUK7SWGpwGlSRJ087ANg1aSkWDmiRJOmG8h02SJCnnDGySJEk5Z2CTJEnKOe9hy4HBcoXBcpVZpSKzfNiuJEk6jOlgBvUMlA8+FiRRfzDI6uXtrFvlY0EkSdLrDGzTYCojZD0DZT53/1YGyhUWzW6mWAiqtcTj23vZ2r2f2y5faWiTJEmAge24HM8I2cYtuxkoV1gyt+VgrVgIlsxtYVffEBu37OaGi88+sRcgSZJOCi46mKIDI2SPb+9l0exmzprbwqLZzTy+vZfPf28rPQPlI+47WK4c3G88B44zVK6eqO5LkqSTiIFtisaOkBULAbw+QtY/UmHjlt1H3HewXCVl7cdTrwcD5coJ6LkkSTrZGNim4HhHyGaVigQcfGn84er1RKsrRiVJEga2KTneEbJZpQZWL2+ne//wuNu79w+zenm7r7uSJEmAgW1KpmOEbN2qxbQ1NbCrb+jgcaq1xK6+IdqaGli3avEJ6LkkSToZGdimYDpGyOa3lrjt8pUHj7Orb5ju/cOsObvdR3pIkqRDeJPUFK1btZit3fvZ1Td0yHPUuvcPT3iEbH5riRsuPptrVy9loFyhtdTgNKgkSXqD4xphi4gXIuLJiNgcEV1ZbX5EbIqIrdnP9qweEfHZiNgWET+JiDVjjrM+a781ItaPqV+YHX9btu/4N43NgOkcIWspFTmjrcmwJkmSxhUpjX8f1oR2jngB6EwpvTqm9h+AnpTS7RHxCaA9pfS7EXE18HHgauBi4L+klC6OiPlAF9AJJOBR4MKUUm9E/Bj4TeCHwHeAz6aU/vFofers7ExdXV1TvqapGCpXHSGTJEmTFhGPppQ6j9XuRNzD9n5gQ/Z9A3DtmPqdqe6HwLyIWAKsAzallHpSSr3AJuCqbNuclNLDqZ4q7xxzrFxJpEN+SpIkTafjvYctAd+NiAT8VUrpS8CZKaVdACmlXRGxKGu7FNg+Zt8dWe1o9R3j1HPDl7dLkqQ3w/EGtn+RUno5C2WbIuKnR2k73v1naQr1Nx444hbgFoCzz35z3r/py9slSdKb5bimRFNKL2c/u4FvAu8CXsmmM8l+dmfNdwDLx+y+DHj5GPVl49TH68eXUkqdKaXOhQsXHs8lTdjxvJpKkiRpMqYc2CKiNSJmH/gOXAlsAe4FDqz0XA98O/t+L/CRbLXoWqAvmzrdCFwZEe3ZitIrgY3Ztv0RsTZbHfqRMceaUb68XZIkvZmOZ0r0TOCb2ZM2GoC7Ukr3RcQjwD0RcRPwEnBd1v471FeIbgMGgY8CpJR6IuIzwCNZuz9OKfVk338N+BrQAvxj9plxh7+aarRaY7Rao7FYoLFYOOTVVK4alSRJx2vKgS2l9DzwC+PU9wLvHaeegN84wrG+AnxlnHoXsGqqfTxRDryaqn+4ws/3DrC7b+jgtsVzWzh3QSu+vF2SJE0XX001BbNKDbx90Wwe3LqH3X1DtDY1MLu5kdamBnb3DfHg1j28fdFsR9ckSdK0cAhoquL1Zay1WqJKOvh7HPwPSZKk4+cI2xQMlis8+8p+Ljp3PrVaYmv3frZ197O1ez+1lLjo3Pk8+8p+Fx1IkqRpYWCbgsFylZHRGk+81Mu+4dFssUHQWCywb2iUJ7b3MlKpMVCuzHRXJUnSKcAp0SmYVSrywt5+dvYNUSwUaGooEBGklChXEzv3DVEshIsOJEnStHCEbYp6B0epJWhqKJCAaq3+JtGmhgK1VN8uSZI0HRwCmoK9/WUS0FgI9g2NUqu9/sasQiFoaajn4L0DIywrzZqhXkqSpFOFI2xTkogEEUEkqD9irv7zQJ3EEd58KkmSNDmOsE3BgrYmCChXa8yd1UgtC20RQSGgf6T+hoMFbU0z3VVJknQKcIRtitpnlSgAI5UaQf01VZH9Xsi2S5IkTQdH2KZgsFzlnAWtVGuJvQNlypUqREBKNDYUWTynmXPOaPVdopIkaVoY2KZgVqlIU2OBi9+6gBdeHWDnvqGDU6JL57Vwzhmt7B8e9bEekiRpWpgopmBWqYHVy9t5fHsvq5bO5R2LZ1Ou1igVCzQUC+zqG2L18nZH1yRJ0rTwHrYpWrdqMW1NDezqG6KSPdajUkvs6huiramBdasWz3APJUnSqcLANkXzW0v86tq3MFqpsfGp3Wx6+hU2PrWb0UriV9e+hfmtLjqQJEnTw8A2RT0DZf77wy/S2FDg8ncs4t1vO4PL37GIxobgf/zwRXoGyjPdRUmSdIrwHrYp2rhlNz0DI+wfqbJlZ9/B+uK5LYyMVtm4ZTc3XHz2DPZQkiSdKgxsUzBYrvDD51/l53sHqVRrtDY1UIiglhK7+4Z4tVgAXuXa1UtdeCBJko6bU6JTMFiusmPfEJVqjdnNjVRricFyhWotMbu5kUq1xo59wwyUKzPdVUmSdApwhG1KEnv7y7SWimzr7qd/ZPTglramRhbPaWJv/wgFYgb7KEmSThWOsE1J0NpUZNueelhrKAQNhQINhaB/ZJRte/ppaypS8+3vkiRpGjjCNgWzSkX6ButToBHBSKV2cFtEkFJi32DFNx1IkqRp4QjbFAyWKwyMVCgWCpAgZQNpKQEJioUCAyMVhsrVGe2nJEk6NTgENAU9A6MQiQAaGwqvJzaACGq1BAGvDowwv80H6EqSpONjYJuClsYCo5VEW1OB/nKV0crrga2xIWhrKjI8WnNKVJIkTQunRKegpdTA3FmNvDZcpRAFmorQ1BA0FaEQBV4brjJvViNNjf55JUnS8TNRTMGsUpF5sxpJKTEyWqVcC8rVRLkWjIxWSSkxd1ajI2ySJGlaGNimqKEQzCo1UCzU71lLCWq1RLFQYFapgYaCz2CTJEnTwyGgKRgsV4GgWAhmN9dfSwX1sFappfqiA4KBcsVXU0mSpONmYJuSRM9AmVmlIr2Do4xW689haywGC1qbaG8t0Tc06psOJEnStDCwTcG+wVGGRqv0D1eo1GrUsmexVVOid7BMY0NwRluTbzqQJEnTwnvYpmDT068wVK4yWq2REhQDGooBCQZGquzaN8wZs0suOpAkSdPCRDFJg+UK/+uZVygEFLOFBdVsiC0CIqBSSzQWwvvXJEnStHCEbZL29o/w6v4RioUCKSVGq4lqov7JXimaEgyP1nw1lSRJmhYGtkkLKtXE4MgolRqH3KWW4PVa1FeJSpIkHS+nRCdpQVuJRKJcO3KbSi2xu2/Ie9gkSdK0cIRtCqoTWPz5St/wie+IJEk6LRjYJmlv/wgDI8ee6tzTX2bvwMib0CNJknSqM7BN0lC5xuDIsRcTJGD3PkfZJEnS8fMmq0kaHq1wlNvXDvHMrj7OnNvEy33DnDW3meXzW09o3yRJ0qnJwDZJKSb+uqn/cv9Wbr/vZ9TH24KVi9r4o391Pmve0n7C+idJkk49TolO0vxZjRNu2zMwSrVapZagWq3yzK79/OuvPcJjL/aewB5KkqRTjYFtklpKDZQmOMiWgGKxSEMhaGgoUijAQLnCH317ywntoyRJOrUY2CYtUZ7CO90DaCgERRLPdu9nZ+/QtPdMkiSdmgxsk9QzUH5DrYVhFtBHC4euCk3U3y06VkNDkVoNnu/efwJ7KUmSTiUuOpik3sHRg9/beY0rC12sKWzjwMKCx2pv47u1TnqZQ4H6yNpYqVYfnmtvK71ZXZYkSSc5A9skRcoCF69xW8O3aGWYV5hHjQIFaqwubOPthR18vnIt+5jzhv3LtcTs5gZWLJz9ZnddkiSdpJwSnaR9Q/Up0SsLXbQyzG7mU8v+jDUK7GY+rQxzZaGLBNSyEbVaLTFSqRHA9Z3LaSkVZ+gKJEnSycbANkl7+su0MMyawja6mUcDFVoYpoHXX1fVzTzWFLbRwgjlWmK4UqNcS7Q1NXDFeYu4+T0rZvAKJEnSycYp0UmKlJjFCCXKvDNe5Kx4lQKJGsHL6QyeS2cxTBOJxC+cEcxfsojBkRqzmgpcunIRV56/mPmt3r8mSZImzsA2SbObGyhR5oJ4npYo00iFRBAk5sYAi+jlR+mdBEHvaCNfvW41A+UKraUGp0ElSdKUOCU6SXNmNXFV4RFaGa5PeUYDo9FAORpopMpZsZfOeJbHam/jpdeCllKRM9qajhjWBssVXu0fYbBcGXe7JEmSI2yT1JqGeV/xx+yjlXkxSIkqoxQJEpUo0pIqLIlXebD2zznao3F7Bsps3LKbx7f3Zg8EgdXL21m3yilTSZJ0KAPbJP38pRe4MPrYwzxGUhNL2cOCGCABiaCfZkZppMCRX4fQM1Dmc/dvZaBcYdHsZoqFoFpLPL69l63d+7nt8pWGNkmSdJBTopP00937gaCJMueyi7kxSAMVGqlSpEoLZeYywBz6gfGnPDdu2c1AucKSuS0UC/VH6xYLwZK5LfSPVNi4ZfdMXNqEOIUrSdKbzxG2Sdq2v4l9qZV/Fi/RFKNj3mSQKJJoiTLl1MAlhSd5pnYu/+ddj1GjPuW59twz+MV3LOLh519lydyWcY+/aHYzj2/v5drVS3O1SMEpXEmSZo6BbZJ+9NIIPQ1tNMfouMOTiQQxyseK97KvVuKen151MNQ98LM93PG/fsaclkbmt5ZYMreFFYvaaCgEo9UajcUCjcX6C60GypXcBDancCVJmlkGtkkKhjkndr3hpe6vb4cWoDkGub10J7dzJ18vX8YnuYVSGqZldIT+ShMLWheyo3eQn72yn5bGAhFBIYIlc5uZ09JIa+nI/9UMlisMlqvMKhWZdZR202XsFO4BB6Zwd/UNsXHLbm64+OwT3g9Jkk5XBrZJms9rnB3db3ip++EOBLqU4MOlB3h7ZTsvFZZz8CXxr7yNh4oX0V2ZTWXM+oQdPQOsWDibodEqiXRIMNv01Mt8+4ldPLvrNUYqNebNKvFLv3AWa9+6gFpKBNDeWqKl1HDEMDc27NV/rzJUrjA0WmN+ayOzSg2HnPNff+Vh/r9tPSya08i/6jg7GwF83aLZzTzyQg+XvP0M5reWGCxX6BkYZX5rI2e0NQOwvWeAl/uGOWtuM8vnt07p7/5mGy8Uv9o//IZrm6xnd7/GCz2DnDN/Fm9f/MZ3zR6rD5Kk01OkdOTVjHkQEVcB/wUoAv8tpXT70dp3dnamrq6uE9af//0T/5FvlD5zxBG28Rz4E38v/QLb0jLKNLKIfQzQzOcr19I7zkviW0vBxW89gwVtTTzX3c9Ptu87JNgdrkA9JBYLwaLZTaxaNpf3rFx08B6zsfegDY9W2blviJHRKvsGR9k3OEpjQwFIzG5u5Lwlc3jgp68wXH3jeUoBN17yVloaiwyNVnmuu5/n9/RzzhmtPLenn4GRCrOaGihG8Jb5s3jltWF29L7+gJO3LWrjj/7V+ax5S/vE/4BvovHu1VvW3sJPd+3n6V2vHWy3evk8bnvvSlYsbJvQcR/4aTf//ltPsvu1kfo/EBEsntPEn1x7AZe9Y9Ex++D9gvkwHaFdksaKiEdTSp3HbJfnwBYRReBZ4ApgB/AIcENK6ekj7XOiA9uffWI9v1361qQC2wEvpQXsZR4/rr2DYZpYTA+P197G39YuH7d9Y8A/XzaXR7f3Tej4xahngYZi0NbUwMVvXcCSuc386tq38N8ffpGBcoXZTY08+mIPrw1X2NE7AARzmhvoH6mns8aGYP94SW3seYBf/d/ewk+272OkUmWkmugbLFOpJRoiaCgWmN1c5Pk9gxAwf1YjTY1FqrUa/SNVSsUC/239RbkLbePdq/dq/wj/8JOXqSV46xmtNDcWqdRq9AyUaWks8rn/Y80xQ9sDP+3m3/z3R6nWapSKQbFQoFqrUa4mioUCf/WhCw+GtiPdL9i9f5i2pgbvF5whz+3p5/P3b+Xx7fsO1iYb2iWdXN6sWY6JBra8P9bjXcC2lNLzKaUycDfw/pns0L/mW1Pet4VhSlRYES8Dr78kvpmRcduPJiYc1qA+2Voo1H8OjVb5+Z5++kcqfD4LAEvmtvDzvQOUqzX6hkaJCIqFYKBcJQIKhfp9dMdSBX78/F7K1RoRweBIhWr2cvvmUpFqSrzUMwRRHx0ayB4BUiwUmNvSSLla44//n6cmfF1vlvEet/Loi70E0NRQoG9oFICGQoFFs5sZGq3y+fu3HvO4//5bT1Kt1WhpLFIs1P8nVywUaMlC7L//1pNH7cPJ8siXU9Vze/r5+F2PsXnHPha0lThzTjML2kps3rGPj9/1GM/t6Z/pLkqaRj0DZf7mRy/xqXuf4s/u+ymfuvcp/uZHL9EzUJ7RfuU9sC0Fto/5fUdWmzGzjmPfKiMM0MyS6KGBKjUKJBKtDE9L32qJ+ltNUyKlxK6+YZobizy+fR/ts0qMVmvs7hui1FCgf2SUxkKBQgTlSo1CBAUSfUMTe77aT1/pp5qgENmK1sbXV7QGUK4mClGfpi1XEtXa6yO5bU1FtnX3s7P3aO+CeHMNlis8vr2XRbObD6nt6huipbFIqVigPwumB8xvLfH49n309B/5f8TP7n6N3a+NUCqOH4RLxWD3ayM8190/bh/GOvDIl6Hy0UdANb0+f/9WhkarLJrdTEMWuCcb2iWdHA7Mchz4/+Kz5rYc/P/ez39v64yGtrwHtvH+LfeGOdyIuCUiuiKia8+ePW9Ct6amjfrbEAAaqVCgRhAMMP33wkQECQ4+4LaWEqPVGsDB0HH4YFpMcp53UVuJdyyeXQ97hTFPpMum2YPXR+zGBp36KFOwY9/gpM53Ig2WqyQ4OKoFHAxGhUIc/FvVxtxC0FAoEASvDow/QgrwQs8gpHRwZO1wB+rPvdo/bh8ObRsceOSL3hyv9g/z+PZ9R5yGnkhol3TyyPMsR94D2w5g+ZjflwEvH94opfSllFJnSqlz4cKFJ7RDxzP50cNsIsubozSwiH08VnsbwzRNT+fGSNmq0QPz7oWIgys8D/xDePjti5O5n7EEnHfWXNqzf5HVxgSyA8EvkQ4GnLEhpFqrAYll845nvHJ6zSoVCQ4Nlgeeg1erpYN/q7FTxpVajUTijNYj//d3zvxZEJFd8xsdqK84o23cPhzatv4CtKM98kXTq2fg9Wnw8UwktEs6OeR9liPvge0RYGVEnBsRJeB64N6Z7NCZt9fvKZvMWo2Utd/NIloZZneaxxn0MUAz360d+T7Dya5rKEY9JEUEkT3TbXi0yurl8+gdLNNYLLB4bgvlSo22pkZGazVqKVFqKFBLiRrB3JaJhYFrOs6id7DMrFIDS+a2MDT6+j/AifpUXy3Vr7vUEIcEtv6RKm9b1MbS9vHf9jATZpUaWL28ne79w4fUDlxbuVqjranhkOvoGSizevk85rcdeRHA2xfPYfGcJsrV8f+BKVcTi+c0sWJR27h9GKt7/zCrl7fn5oHKp4P5rY1APZyPZyKhXdLJIe+zHLkObCmlCnAbsBF4BrgnpTTjd6sPZrMfEwltB5r0pGbOoI9GKuxPrTxWe9sRH+lxQGMBLlw+d1J9q9Wyh/c2Fjl3YVt9ZeF7V9LW1MCuviHOXdBKqVi/+T+l+r1lraUiKdVHkmoTuKiWBg455oVvaachu8druFylGMH/3969xtZd13Ecf3/brl23srGtXMcuBDaEDXWIoD5QLiKXKMMEdIskEhcSFPABRENiNEaIDzQGH3iJGI2XKIg8UKIoJsKUEFiAcFFGgHEvEAZsHazr2q79+uCc1q7r2v/punP+He/Xo3N6fud/vmffnX8//f3//99ZurAdsvL+h2eEBocqFzu0Njfxrc+squl91cP5q48eeU/Ds1wfql7J2rdniPnt///lvfXd3bTPauaac1dMut2bLjmV5qYmegcGR2bUBoeG6B0YpLmpiZsuOXXCGgaHktd39NLR1sL5q4+e1vesiXV2zGbNksP3e95KkdAuaWYo+1GOUi/rMRUHe1mPYTtvmM+cgvvorUNz+Bdn8MTQch4YOoXXOGLSw6BHHdbKqsXz6eyYzZat7/Kfrm4Gxv8jH6gstUF1Hbaj5s1m1eJ5nLXySD61at912PoGhujq3lVdh20P3bv6R9Zhmzd7FicfO4+NT42/Dtu8Ntj49fP22eY7uwfY/No79PTtYW5bC00RLFs0hzd2DK/DFkCy4qgOvvnpmbEO23DNSxa081R1HbYgSJLTli7g6nNOnNo6bFXHzG/jxrUTr8M2XMNpSxeM9FL1NXyVaO/AIAvnttLS1FTz0i6SZoZbN73Mo69sH/f7vl/f0cuaJQum/Zt9Dol12KaiXoFt2Os3zB+ZI3uTJr7LtVzIRk7kDf7BafycyziM3bzL7L1C2izg+vNXMmd2C3dseomn3+jh+EXtXH/RyZxyzOEsXtBOb/8gPf17mNvaQntrM4+8uI0nXt3Bjp4+uror/3EuPPVYdg3sobunn0xY2NFG26ymkeeMNXqbUFlyo29giJ7+PXTObaO9tXmv1/zq7x7i/ue2cfqy+fxw/Ycn3WZv/yBv9fTRObdtZNbh1e29dHXv4rjD55TqMOhExv7bA2zb2b/Pe6vVc1t38txbOzmhs4MTjpz4l/x4NagxRq/DNtXQLqn8tvX086N7nmVnX/3WwjSwSdI0m47QLqnc6n2Uo2hg83IzSSpoYUerQU06xC2c28r6M5dyyZrFpTrKYWCTJEkao721uRRBbViprxKVJEmSgU2SJKn0DGySJEklZ2CTJEkqOQObJElSyRnYJEmSSs7AJkmSVHIGNkmSpJIzsEmSJJWcgU2SJKnkDGySJEklZ2CTJEkqOQObJElSyRnYJEmSSs7AJkmSVHIGNkmSpJIzsEmSJJWcgU2SJKnkDGySJEklZ2CTJEkqucjMRtcwrSLiTeClBrx0J/BWA15XB87ezWz2b2azfzOb/TtwyzLziMkGHXKBrVEi4uHMPL3Rdah29m5ms38zm/2b2exf/XhIVJIkqeQMbJIkSSVnYJs+tzS6AE2ZvZvZ7N/MZv9mNvtXJ57DJkmSVHLOsEmSJJWcga0GEXFBRDwdEVsi4oZxHm+LiD9UH98UEcvrX6X2p0D/rouIzRHxRET8MyKWNaJOjW+y/o0ad2lEZER45VqJFOlfRHyu+hl8MiJ+X+8atX8F9p9LI+LeiHi0ug+9qBF1Hso8JFpQRDQDzwDnAV3AQ8D6zNw8asxXgPdn5lURsQ74bGZ+viEFay8F+3c2sCkzd0XEl4Gz7F85FOlfddxhwF+BVuCazHy43rVqXwU/fyuA24FzMnN7RByZmVsbUrD2UrB/twCPZuZPI+IU4K7MXN6Ieg9VzrAVdwawJTOfz8x+4DZg7Zgxa4FfV2/fAZwbEVHHGrV/k/YvM+/NzF3Vuw8Cx9W5Ru1fkc8fwI3A94Dd9SxOkyrSvyuBH2fmdgDDWqkU6V8C86q35wOv1bG+9wQDW3GLgVdG3e+q/mzcMZm5B9gBLKpLdZpMkf6NtgH420GtSLWYtH8RsQZYkpl/qWdhKqTI528lsDIi7o+IByPigrpVp8kU6d+3gcsjogu4C7i2PqW9d7Q0uoAZZLyZsrHHk4uMUWMU7k1EXA6cDnzioFakWkzYv4hoAm4GrqhXQapJkc9fC7ACOIvK7PZ9EbE6M7sPcm2aXJH+rQd+lZk/iIiPAr+t9m/o4Jf33uAMW3FdwJJR949j3ynfkTER0UJlWnhbXarTZIr0j4j4JPAN4OLM7KtTbZrcZP07DFgNbIyIF4GPAHd64UFpFN1//jkzBzLzBeBpKgFOjVekfxuonINIZj4AzKbyPaOaJga24h4CVkTE8RHRCqwD7hwz5k7gi9XblwL3pFd1lMWk/aseUvsZlbDm+TPlMmH/MnNHZnZm5vLqic4PUumjFx2UQ5H955+AswEiopPKIdLn61ql9qdI/14GzgWIiJOpBLY361rlIc7AVlD1nLRrgLuBp4DbM/PJiPhORFxcHfYLYFFEbAGuA/a79IDqq2D/vg90AH+MiMciYuwOSQ1SsH8qqYL9uxt4OyI2A/cCX8vMtxtTsUYr2L/rgSsj4nHgVuAKJyyml8t6SJIklZwzbJIkSSVnYJMkSSo5A5skSVLJGdgkSZJKzsAmSZJUo4j4ZURsjYj/Fhh7c3X1gcci4pmIqHlBaK8SlSRJqlFEfBzYCfwmM1fX8LxrgTWZ+aVaXs8ZNkmSpBpl5r8Z821GEXFCRPw9Ih6JiPsi4n3jPHU9lbXqauJ3iUqSJE2PW4CrMvPZiDgT+AlwzvCDEbEMOB64p9YNG9gkSZIOUER0AB+j8m05wz9uGzNsHXBHZg7Wun0DmyRJ0oFrAroz84MTjFkHXD3VjUuSJOkAZOY7wAsRcRlAVMvfoY8AAAB0SURBVHxg+PGIOAlYADwwle0b2CRJkmoUEbdSCV8nRURXRGwAvgBsiIjHgSeBtaOesh64Lae4PIfLekiSJJWcM2ySJEklZ2CTJEkqOQObJElSyRnYJEmSSs7AJkmSVHIGNkmSpJIzsEmSJJWcgU2SJKnk/gc43Mr11D3KcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# Fit and transform x to visualise inside a 2D feature space\n",
    "x_vis = pca.fit_transform(X_train)\n",
    "\n",
    "def plot_two_classes(X, y, subplot=False, size=(10, 10)):\n",
    "    # Plot the two classes\n",
    "    if subplot == False:\n",
    "        fig, subplot = plt.subplots(nrows=1, ncols=1, figsize=size)\n",
    "        \n",
    "    subplot.scatter(X[y==0, 0], X[y==0, 1], label=\"Class #0\", \n",
    "                    alpha=0.5, s=70)\n",
    "    subplot.scatter(X[y==1, 0], X[y==1, 1], label=\"Class #1\", \n",
    "                    alpha=0.5, s=70)\n",
    "    subplot.legend()\n",
    "    return subplot\n",
    "\n",
    "plot_two_classes(x_vis, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0057453449730033666"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnderSampling(X, y, target_percentage=0.5, seed=None):\n",
    "    # Assuming minority class is the positive\n",
    "    n_samples = y.shape[0]\n",
    "    n_samples_0 = (y == 0).sum()\n",
    "    n_samples_1 = (y == 1).sum()\n",
    "\n",
    "    n_samples_0_new =  n_samples_1 / target_percentage - n_samples_1\n",
    "    n_samples_0_new_per = n_samples_0_new / n_samples_0\n",
    "\n",
    "    filter_ = y == 0\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    rand_1 = np.random.binomial(n=1, p=n_samples_0_new_per, size=n_samples)\n",
    "    \n",
    "    filter_ = filter_ & rand_1\n",
    "    filter_ = filter_ | (y == 1)\n",
    "    filter_ = filter_.astype(bool)\n",
    "    \n",
    "    return X[filter_], y[filter_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target percentage 0.1 Accuracy 0.9935988004959488 F1 score 0.0 F Beta score 0.0\n",
      "Target percentage 0.2 Accuracy 0.9705602491277645 F1 score 0.046685340802987856 F Beta score 0.12213998935809993\n",
      "Target percentage 0.3 Accuracy 0.9352959833914823 F1 score 0.0352536543422184 F Beta score 0.1888453119299526\n",
      "Target percentage 0.4 Accuracy 0.8670453562469363 F1 score 0.031912660088179716 F Beta score 0.31504206854093986\n",
      "Target percentage 0.5 Accuracy 0.5890256913007123 F1 score 0.017779615464130656 F Beta score 0.38193650514466626\n"
     ]
    }
   ],
   "source": [
    "for target_percentage in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    X_u, y_u = UnderSampling(X_train, y_train, target_percentage, 1)\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_u, y_u)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    performance_scores.append(['Under sampling','Logistic Regression',target_percentage,'No k',metrics.accuracy_score(y_pred, y_test), metrics.f1_score(y_pred, y_test), metrics.fbeta_score(y_pred, y_test,10)])   \n",
    "    print('Target percentage', target_percentage,'Accuracy',metrics.accuracy_score(y_test, y_pred),'F1 score',metrics.f1_score(y_test, y_pred), 'F Beta score',metrics.fbeta_score(y_test, y_pred, beta = 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target percentage 0.1 Accuracy 0.9884086387359072 F1 score 0.13733905579399144 F Beta score 0.16105242176599563\n",
      "Target percentage 0.2 Accuracy 0.9709639283757677 F1 score 0.076993583868011 F Beta score 0.204996858841154\n",
      "Target percentage 0.3 Accuracy 0.9349788068394799 F1 score 0.05845511482254698 F Beta score 0.32140746465427106\n",
      "Target percentage 0.4 Accuracy 0.8305123843026441 F1 score 0.03576115485564304 F Beta score 0.4283990972060083\n",
      "Target percentage 0.5 Accuracy 0.8190651941985525 F1 score 0.03742905353581837 F Beta score 0.47172772864744833\n"
     ]
    }
   ],
   "source": [
    "for target_percentage in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    X_u, y_u = UnderSampling(X_train, y_train, target_percentage, 1)\n",
    "    clf = DecisionTreeClassifier(max_depth = 4, random_state = 42)\n",
    "    clf.fit(X_u, y_u)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    performance_scores.append(['Under sampling','Decision Tree',target_percentage,'No k', metrics.accuracy_score(y_pred, y_test), metrics.f1_score(y_pred, y_test), metrics.fbeta_score(y_pred, y_test,10)])   \n",
    "    print('Target percentage', target_percentage,'Accuracy',metrics.accuracy_score(y_test, y_pred),'F1 score',metrics.f1_score(y_test, y_pred), 'F Beta score',metrics.fbeta_score(y_test, y_pred, beta = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target percentage 0.1 Accuracy 0.9933969608719472 F1 score 0.12927756653992395 F Beta score 0.08643342562295495\n",
      "Target percentage 0.2 Accuracy 0.9889853233759118 F1 score 0.16593886462882096 F Beta score 0.19132602193419737\n",
      "Target percentage 0.3 Accuracy 0.9718866237997751 F1 score 0.12082957619477007 F Beta score 0.32673458548597367\n",
      "Target percentage 0.4 Accuracy 0.8568380381188547 F1 score 0.04610951008645533 F Beta score 0.4885717740960213\n",
      "Target percentage 0.5 Accuracy 0.7933450592543467 F1 score 0.035007405412683455 F Beta score 0.4857745384586925\n"
     ]
    }
   ],
   "source": [
    "for target_percentage in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    X_u, y_u = UnderSampling(X_train, y_train, target_percentage, 1)\n",
    "    clf = RandomForestClassifier(max_depth = 4, random_state = 42, n_estimators=50)\n",
    "    clf.fit(X_u, y_u)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    performance_scores.append(['Under sampling','Random Forest',target_percentage,'No k',metrics.accuracy_score(y_pred, y_test), metrics.f1_score(y_pred, y_test), metrics.fbeta_score(y_pred, y_test,10)])   \n",
    "    print('Target percentage', target_percentage,'Accuracy',metrics.accuracy_score(y_test, y_pred),'F1 score',metrics.f1_score(y_test, y_pred), 'F Beta score',metrics.fbeta_score(y_test, y_pred, beta = 10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 15.3\n",
    "\n",
    "Same analysis using random-over-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def OverSampling(X, y, target_percentage=0.5, seed=None):\n",
    "    # Assuming minority class is the positive\n",
    "    n_samples = y.shape[0]\n",
    "    n_samples_0 = (y == 0).sum()\n",
    "    n_samples_1 = (y == 1).sum()\n",
    "\n",
    "    n_samples_1_new =  -target_percentage * n_samples_0 / (target_percentage- 1)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    filter_ = np.random.choice(X[y == 1].shape[0], int(n_samples_1_new))\n",
    "    # filter_ is within the positives, change to be of all\n",
    "    filter_ = np.nonzero(y == 1)[0][filter_]\n",
    "    \n",
    "    filter_ = np.concatenate((filter_, np.nonzero(y == 0)[0]), axis=0)\n",
    "    \n",
    "    return X.iloc[filter_], y.iloc[filter_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target percentage 0.1 Accuracy 0.9942908220639544 F1 score 0.0 F Beta score 0.0\n",
      "Target percentage 0.2 Accuracy 0.9847178570398778 F1 score 0.01851851851851852 F Beta score 0.02507198887895939\n",
      "Target percentage 0.3 Accuracy 0.946627836567573 F1 score 0.03844155844155844 F Beta score 0.17359594927300598\n",
      "Target percentage 0.4 Accuracy 0.9046163605432369 F1 score 0.03837209302325581 F Beta score 0.2892978040100685\n",
      "Target percentage 0.5 Accuracy 0.5807791009486463 F1 score 0.018762232570695824 F Beta score 0.4078851796972602\n"
     ]
    }
   ],
   "source": [
    "for target_percentage in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    X_u, y_u = OverSampling(X_train, y_train, target_percentage, 1)\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_u, y_u)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    performance_scores.append(['Over sampling','Logistic Regression',target_percentage,'No k', metrics.accuracy_score(y_pred, y_test), metrics.f1_score(y_pred, y_test), metrics.fbeta_score(y_pred, y_test,10)])   \n",
    "    print('Target percentage', target_percentage,'Accuracy',metrics.accuracy_score(y_test, y_pred),'F1 score',metrics.f1_score(y_test, y_pred), 'F Beta score',metrics.fbeta_score(y_test, y_pred, beta = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target percentage 0.1 Accuracy 0.9909460511519276 F1 score 0.1229050279329609 F Beta score 0.11132264529058115\n",
      "Target percentage 0.2 Accuracy 0.9699258960237594 F1 score 0.08105726872246695 F Beta score 0.22404397936056325\n",
      "Target percentage 0.3 Accuracy 0.9427640494795421 F1 score 0.061465721040189124 F Beta score 0.3022977390984022\n",
      "Target percentage 0.4 Accuracy 0.8431417779187451 F1 score 0.04225352112676056 F Beta score 0.4793924531287082\n",
      "Target percentage 0.5 Accuracy 0.8431417779187451 F1 score 0.04225352112676056 F Beta score 0.4793924531287082\n"
     ]
    }
   ],
   "source": [
    "for target_percentage in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    X_u, y_u = OverSampling(X_train, y_train, target_percentage, 1)\n",
    "    clf = DecisionTreeClassifier(max_depth = 4, random_state = 42)\n",
    "    clf.fit(X_u, y_u)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    performance_scores.append(['Over sampling','Decision Tree',target_percentage,'No k',metrics.accuracy_score(y_pred, y_test), metrics.f1_score(y_pred, y_test), metrics.fbeta_score(y_pred, y_test,10)])   \n",
    "    print('Target percentage', target_percentage,'Accuracy',metrics.accuracy_score(y_test, y_pred),'F1 score',metrics.f1_score(y_test, y_pred), 'F Beta score',metrics.fbeta_score(y_test, y_pred, beta = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target percentage 0.1 Accuracy 0.9937718058879502 F1 score 0.13599999999999998 F Beta score 0.08649002619383436\n",
      "Target percentage 0.2 Accuracy 0.9902828638159222 F1 score 0.16790123456790124 F Beta score 0.1716399260258909\n",
      "Target percentage 0.3 Accuracy 0.9719731264957758 F1 score 0.12274368231046931 F Beta score 0.33162723322066634\n",
      "Target percentage 0.4 Accuracy 0.873129379198985 F1 score 0.04844290657439446 F Beta score 0.4669363493767027\n",
      "Target percentage 0.5 Accuracy 0.8000634353104005 F1 score 0.036944444444444446 F Beta score 0.5011939407506902\n"
     ]
    }
   ],
   "source": [
    "for target_percentage in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    X_u, y_u = OverSampling(X_train, y_train, target_percentage, 1)\n",
    "    clf = RandomForestClassifier(max_depth = 4, random_state = 42, n_estimators=50)\n",
    "    clf.fit(X_u, y_u)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    performance_scores.append(['Over sampling','Random Forest',target_percentage,'No k', metrics.accuracy_score(y_pred, y_test), metrics.f1_score(y_pred, y_test), metrics.fbeta_score(y_pred, y_test,10)])   \n",
    "    print('Target percentage', target_percentage,'Accuracy',metrics.accuracy_score(y_test, y_pred),'F1 score',metrics.f1_score(y_test, y_pred), 'F Beta score',metrics.fbeta_score(y_test, y_pred, beta = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 15.4 (3 points)\n",
    "\n",
    "Evaluate the results using SMOTE\n",
    "\n",
    "Which parameters did you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intentar dos o tres combinaciones de parámetros porque las iteraciones toma mucho tiempo. No hacer grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOTE(X, y, target_percentage=0.5, k=5, seed=None):\n",
    "    n_samples = y.shape[0]\n",
    "    n_samples_0 = (y == 0).sum()\n",
    "    n_samples_1 = (y == 1).sum()\n",
    "    \n",
    "    n_samples_1_new =  int(-target_percentage * n_samples_0 / (target_percentage- 1) - n_samples_1)\n",
    "    \n",
    "    # A matrix to store the synthetic samples\n",
    "    new = np.zeros((n_samples_1_new, X.shape[1]))\n",
    "    \n",
    "    # Create seeds\n",
    "    np.random.seed(seed)\n",
    "    seeds = np.random.randint(1, 1000000, 3)\n",
    "    \n",
    "    # Select examples to use as base\n",
    "    np.random.seed(seeds[0])\n",
    "    sel_ = np.random.choice(y[y==1].shape[0], n_samples_1_new)\n",
    "    \n",
    "    # Define random seeds (2 per example)\n",
    "    np.random.seed(seeds[1])\n",
    "    nn__ = np.random.choice(k, n_samples_1_new)\n",
    "    np.random.seed(seeds[2])\n",
    "    steps = np.random.uniform(size=n_samples_1_new)  \n",
    "\n",
    "    # For each selected examples create one synthetic case\n",
    "    for i, sel in enumerate(sel_):\n",
    "        # Select neighbor\n",
    "        nn_ = nn__[i]\n",
    "        step = steps[i]\n",
    "        # Create new sample\n",
    "        new[i, :] = X[y==1].iloc[sel] - step * (X[y==1].iloc[sel] - X[y==1].iloc[nn_])\n",
    "        \n",
    "    X = np.vstack((X, new))\n",
    "    y = np.append(y, np.ones(n_samples_1_new))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target percentage 0.1 k 5 Accuracy 0.9913497303999308 F1 score 0.0 F Beta score 0.0\n",
      "Target percentage 0.1 k 10 Accuracy 0.9933392924079467 F1 score 0.0 F Beta score 0.0\n",
      "Target percentage 0.1 k 15 Accuracy 0.9939736455119518 F1 score 0.0 F Beta score 0.0\n",
      "Target percentage 0.2 k 5 Accuracy 0.9779994809838239 F1 score 0.02554278416347382 F Beta score 0.04954623497669855\n",
      "Target percentage 0.2 k 10 Accuracy 0.9692915429197543 F1 score 0.03269754768392371 F Beta score 0.08781336038255325\n",
      "Target percentage 0.2 k 15 Accuracy 0.9719154580317754 F1 score 0.033730158730158735 F Beta score 0.08330907326540513\n",
      "Target percentage 0.3 k 5 Accuracy 0.933248752919466 F1 score 0.035014589412255104 F Beta score 0.19280941775373847\n",
      "Target percentage 0.3 k 10 Accuracy 0.9425622098555405 F1 score 0.025440313111545987 F Beta score 0.12131571652961289\n",
      "Target percentage 0.3 k 15 Accuracy 0.9294714685274358 F1 score 0.03013481363996828 F Beta score 0.1734767673115169\n",
      "Target percentage 0.4 k 5 Accuracy 0.8276001268706208 F1 score 0.02479204045017126 F Beta score 0.29829401935258226\n",
      "Target percentage 0.4 k 10 Accuracy 0.8419595744067356 F1 score 0.027674294837679615 F Beta score 0.31213598003090454\n",
      "Target percentage 0.4 k 15 Accuracy 0.781869034918255 F1 score 0.022988505747126436 F Beta score 0.32872554397513254\n",
      "Target percentage 0.5 k 5 Accuracy 0.6984227675095874 F1 score 0.02115114646700983 F Beta score 0.37682834219302014\n",
      "Target percentage 0.5 k 10 Accuracy 0.7094374441336755 F1 score 0.021555490824351882 F Beta score 0.3749372930671215\n",
      "Target percentage 0.5 k 15 Accuracy 0.7138490816297108 F1 score 0.021494774206270953 F Beta score 0.37012506724045185\n"
     ]
    }
   ],
   "source": [
    "for target_percentage in [0.1,0.2,0.3,0.4,0.5]:\n",
    "    for k in [5, 10, 15]:\n",
    "        X_u, y_u = SMOTE(X_train, y_train, target_percentage,k, 1)\n",
    "        logreg = LogisticRegression()\n",
    "        logreg.fit(X_u, y_u)\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        performance_scores.append(['SMOTE','Logistic Regression',target_percentage,k,metrics.accuracy_score(y_pred, y_test), metrics.f1_score(y_pred, y_test), metrics.fbeta_score(y_pred, y_test,10)])   \n",
    "        print('Target percentage', target_percentage,'k',k,'Accuracy',metrics.accuracy_score(y_test, y_pred),'F1 score',metrics.f1_score(y_test, y_pred), 'F Beta score',metrics.fbeta_score(y_test, y_pred, beta = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target percentage 0.1 k 5 Accuracy 0.9848620281998789 F1 score 0.12935323383084577 F Beta score 0.1949517446176689\n",
      "Target percentage 0.1 k 10 Accuracy 0.9868804244398951 F1 score 0.099009900990099 F Beta score 0.12557815686079474\n",
      "Target percentage 0.1 k 15 Accuracy 0.9888699864479109 F1 score 0.10648148148148147 F Beta score 0.11595288010382349\n",
      "Target percentage 0.2 k 5 Accuracy 0.9788933421758311 F1 score 0.09852216748768472 F Beta score 0.19790339962770648\n",
      "Target percentage 0.2 k 10 Accuracy 0.9719442922637755 F1 score 0.08120868744098207 F Beta score 0.21020279754126137\n",
      "Target percentage 0.2 k 15 Accuracy 0.986995761367896 F1 score 0.0998003992015968 F Beta score 0.12560314380938173\n",
      "Target percentage 0.3 k 5 Accuracy 0.9495400939995963 F1 score 0.06316916488222697 F Beta score 0.27755006986492775\n",
      "Target percentage 0.3 k 10 Accuracy 0.93249906288746 F1 score 0.0417519443307409 F Beta score 0.2336584259469268\n",
      "Target percentage 0.3 k 15 Accuracy 0.9201580115913612 F1 score 0.036869565217391306 F Beta score 0.23815455799261467\n",
      "Target percentage 0.4 k 5 Accuracy 0.9232721086473862 F1 score 0.047261009667024706 F Beta score 0.2976557267247153\n",
      "Target percentage 0.4 k 10 Accuracy 0.9254635102794037 F1 score 0.04081632653061225 F Beta score 0.2491366551554021\n",
      "Target percentage 0.4 k 15 Accuracy 0.8723508549349788 F1 score 0.03277255844439589 F Beta score 0.31328839075230575\n",
      "Target percentage 0.5 k 5 Accuracy 0.9020501138952164 F1 score 0.035217267821641585 F Beta score 0.2708126108203953\n",
      "Target percentage 0.5 k 10 Accuracy 0.7584268043020674 F1 score 0.0242254833449802 F Beta score 0.37264084007379034\n",
      "Target percentage 0.5 k 15 Accuracy 0.8335688128946686 F1 score 0.026644182124789206 F Beta score 0.3125097916340279\n"
     ]
    }
   ],
   "source": [
    "for target_percentage in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    for k in [5, 10, 15]:\n",
    "        X_u, y_u = SMOTE(X_train, y_train, target_percentage,k, 1)\n",
    "        clf = DecisionTreeClassifier(max_depth = 4, random_state = 42)\n",
    "        clf.fit(X_u, y_u)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        performance_scores.append(['SMOTE','Decision Tree',target_percentage,k,metrics.accuracy_score(y_pred, y_test), metrics.f1_score(y_pred, y_test), metrics.fbeta_score(y_pred, y_test,10)])   \n",
    "        print('Target percentage', target_percentage,'k',k,'Accuracy',metrics.accuracy_score(y_test, y_pred),'F1 score',metrics.f1_score(y_test, y_pred), 'F Beta score',metrics.fbeta_score(y_test, y_pred, beta = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target percentage 0.1 k 5 Accuracy 0.9927626077679421 F1 score 0.11307420494699646 F Beta score 0.08126728689967314\n",
      "Target percentage 0.1 k 10 Accuracy 0.9931951212479455 F1 score 0.10606060606060606 F Beta score 0.07117688513037351\n",
      "Target percentage 0.1 k 15 Accuracy 0.9939448112799516 F1 score 0.09482758620689655 F Beta score 0.056014923868105276\n",
      "Target percentage 0.2 k 5 Accuracy 0.9841411723998731 F1 score 0.11290322580645161 F Beta score 0.17480961329245376\n",
      "Target percentage 0.2 k 10 Accuracy 0.986505579423892 F1 score 0.13011152416356878 F Beta score 0.1755213505461768\n",
      "Target percentage 0.2 k 15 Accuracy 0.9851792047518815 F1 score 0.09507042253521127 F Beta score 0.13520079325731282\n",
      "Target percentage 0.3 k 5 Accuracy 0.9660909431677287 F1 score 0.07255520504731862 F Beta score 0.22261619549592718\n",
      "Target percentage 0.3 k 10 Accuracy 0.968253510567746 F1 score 0.07866108786610879 F Beta score 0.22825407510698661\n",
      "Target percentage 0.3 k 15 Accuracy 0.9617369741356939 F1 score 0.0687719298245614 F Beta score 0.2353640557378609\n",
      "Target percentage 0.4 k 5 Accuracy 0.9480983823995848 F1 score 0.05263157894736842 F Beta score 0.23486187331411032\n",
      "Target percentage 0.4 k 10 Accuracy 0.9465701681035725 F1 score 0.054109239407861144 F Beta score 0.2482723435833217\n",
      "Target percentage 0.4 k 15 Accuracy 0.9446671087915574 F1 score 0.05234567901234568 F Beta score 0.24751468072317012\n",
      "Target percentage 0.5 k 5 Accuracy 0.9270493930394164 F1 score 0.05101275318829708 F Beta score 0.30842464523082447\n",
      "Target percentage 0.5 k 10 Accuracy 0.9322972232634584 F1 score 0.04862236628849271 F Beta score 0.27458087902129585\n",
      "Target percentage 0.5 k 15 Accuracy 0.9155445344713243 F1 score 0.04623901009443178 F Beta score 0.31627927490848146\n"
     ]
    }
   ],
   "source": [
    "for target_percentage in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    for k in [5, 10, 15]:\n",
    "        X_u, y_u = SMOTE(X_train, y_train, target_percentage,k, 1)\n",
    "        clf = RandomForestClassifier(max_depth = 4, random_state = 42, n_estimators=50)\n",
    "        clf.fit(X_u, y_u)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        performance_scores.append(['SMOTE','Random Forest',target_percentage,k,metrics.accuracy_score(y_pred, y_test), metrics.f1_score(y_pred, y_test), metrics.fbeta_score(y_pred, y_test,10)])   \n",
    "        print('Target percentage', target_percentage,'k',k,'Accuracy',metrics.accuracy_score(y_test, y_pred),'F1 score',metrics.f1_score(y_test, y_pred), 'F Beta score',metrics.fbeta_score(y_test, y_pred, beta = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 15.5 (3 points)\n",
    "\n",
    "Evaluate the results using Adaptive Synthetic Sampling Approach for Imbalanced\n",
    "Learning (ADASYN)\n",
    "\n",
    "http://www.ele.uri.edu/faculty/he/PDFfiles/adasyn.pdf\n",
    "https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.ADASYN.html#rf9172e970ca5-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN\n",
    "ada = ADASYN(random_state=42)\n",
    "X_u, y_u = ada.fit_resample(X_test, y_test)\n",
    "#print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5248983593321992 F1 score 0.01893420660910985 F Beta score 0.44121768277605294\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_u, y_u)\n",
    "y_pred = logreg.predict(X_test)\n",
    "performance_scores.append(['ADASYN','Logistic Regression','No target','No k',metrics.accuracy_score(y_pred, y_test), metrics.f1_score(y_pred, y_test), metrics.fbeta_score(y_pred, y_test,10)])   \n",
    "print('Accuracy',metrics.accuracy_score(y_test, y_pred),'F1 score',metrics.f1_score(y_test, y_pred),'F Beta score',metrics.fbeta_score(y_test, y_pred, beta = 10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8431994463827456 F1 score 0.03649893692416726 F Beta score 0.41206527766774936\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth = 4, random_state = 42)\n",
    "clf.fit(X_u, y_u)\n",
    "y_pred = clf.predict(X_test)\n",
    "performance_scores.append(['ADASYN','Decision  Tree','No target','No k',metrics.accuracy_score(y_pred, y_test), metrics.f1_score(y_pred, y_test), metrics.fbeta_score(y_pred, y_test,10)])   \n",
    "print('Accuracy',metrics.accuracy_score(y_test, y_pred),'F1 score',metrics.f1_score(y_test, y_pred),'F Beta score',metrics.fbeta_score(y_test, y_pred, beta = 10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.750756898590006 F1 score 0.03007181328545781 F Beta score 0.4746440345093637\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth = 4, random_state = 42, n_estimators=50)\n",
    "clf.fit(X_u, y_u)\n",
    "y_pred = clf.predict(X_test)\n",
    "performance_scores.append(['ADASYN','Random Forest','No target','No k',metrics.accuracy_score(y_pred, y_test), metrics.f1_score(y_pred, y_test), metrics.fbeta_score(y_pred, y_test,10)])   \n",
    "print('Accuracy',metrics.accuracy_score(y_test, y_pred),'F1 score',metrics.f1_score(y_test, y_pred),'F Beta score',metrics.fbeta_score(y_test, y_pred, beta = 10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 15.6 (3 points)\n",
    "\n",
    "Compare and comment about the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacer tabla con índice es el modelo, y los resultados están en la tabla. Todo se prueba sobre test, que no se le hizo muestreo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balancing Method</th>\n",
       "      <th>Model</th>\n",
       "      <th>target_percentage</th>\n",
       "      <th>k</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>FBeta Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unbalanced</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>No target</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.994204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unbalanced</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>No target</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.994233</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>0.393215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unbalanced</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>No target</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.994233</td>\n",
       "      <td>0.029126</td>\n",
       "      <td>0.303607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Under sampling</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.1</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.993599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Under sampling</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.970560</td>\n",
       "      <td>0.046685</td>\n",
       "      <td>0.028858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Under sampling</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.3</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.935296</td>\n",
       "      <td>0.035254</td>\n",
       "      <td>0.019441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Under sampling</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.4</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.867045</td>\n",
       "      <td>0.031913</td>\n",
       "      <td>0.016808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Under sampling</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.589026</td>\n",
       "      <td>0.017780</td>\n",
       "      <td>0.009102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Under sampling</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.1</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.988409</td>\n",
       "      <td>0.137339</td>\n",
       "      <td>0.119713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Under sampling</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.2</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.970964</td>\n",
       "      <td>0.076994</td>\n",
       "      <td>0.047398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Under sampling</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.3</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.934979</td>\n",
       "      <td>0.058455</td>\n",
       "      <td>0.032151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Under sampling</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.4</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.830512</td>\n",
       "      <td>0.035761</td>\n",
       "      <td>0.018659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Under sampling</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.5</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.819065</td>\n",
       "      <td>0.037429</td>\n",
       "      <td>0.019488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Under sampling</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.1</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.993397</td>\n",
       "      <td>0.129278</td>\n",
       "      <td>0.256345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Under sampling</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.2</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.988985</td>\n",
       "      <td>0.165939</td>\n",
       "      <td>0.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Under sampling</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.3</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.971887</td>\n",
       "      <td>0.120830</td>\n",
       "      <td>0.074120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Under sampling</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.4</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.856838</td>\n",
       "      <td>0.046110</td>\n",
       "      <td>0.024197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Under sampling</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.5</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.793345</td>\n",
       "      <td>0.035007</td>\n",
       "      <td>0.018158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Over sampling</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.1</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.994291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Over sampling</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.984718</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.014681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Over sampling</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.3</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.946628</td>\n",
       "      <td>0.038442</td>\n",
       "      <td>0.021614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Over sampling</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.4</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.904616</td>\n",
       "      <td>0.038372</td>\n",
       "      <td>0.020549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Over sampling</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.580779</td>\n",
       "      <td>0.018762</td>\n",
       "      <td>0.009602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Over sampling</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.1</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.990946</td>\n",
       "      <td>0.122905</td>\n",
       "      <td>0.137177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Over sampling</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.2</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.969926</td>\n",
       "      <td>0.081057</td>\n",
       "      <td>0.049479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Over sampling</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.3</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.942764</td>\n",
       "      <td>0.061466</td>\n",
       "      <td>0.034211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Over sampling</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.4</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.843142</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.022101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Over sampling</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.5</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.843142</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.022101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Over sampling</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.1</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.993772</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.318081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Over sampling</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.2</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.990283</td>\n",
       "      <td>0.167901</td>\n",
       "      <td>0.164322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.978893</td>\n",
       "      <td>0.098522</td>\n",
       "      <td>0.065587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.971944</td>\n",
       "      <td>0.081209</td>\n",
       "      <td>0.050326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.986996</td>\n",
       "      <td>0.099800</td>\n",
       "      <td>0.082792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.949540</td>\n",
       "      <td>0.063169</td>\n",
       "      <td>0.035640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.932499</td>\n",
       "      <td>0.041752</td>\n",
       "      <td>0.022924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.920158</td>\n",
       "      <td>0.036870</td>\n",
       "      <td>0.019981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.923272</td>\n",
       "      <td>0.047261</td>\n",
       "      <td>0.025668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.925464</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.022229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.4</td>\n",
       "      <td>15</td>\n",
       "      <td>0.872351</td>\n",
       "      <td>0.032773</td>\n",
       "      <td>0.017291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.902050</td>\n",
       "      <td>0.035217</td>\n",
       "      <td>0.018833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.758427</td>\n",
       "      <td>0.024225</td>\n",
       "      <td>0.012520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.833569</td>\n",
       "      <td>0.026644</td>\n",
       "      <td>0.013915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.992763</td>\n",
       "      <td>0.113074</td>\n",
       "      <td>0.185790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.993195</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.208002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.993945</td>\n",
       "      <td>0.094828</td>\n",
       "      <td>0.308783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.984141</td>\n",
       "      <td>0.112903</td>\n",
       "      <td>0.083377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.986506</td>\n",
       "      <td>0.130112</td>\n",
       "      <td>0.103369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.985179</td>\n",
       "      <td>0.095070</td>\n",
       "      <td>0.073310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.966091</td>\n",
       "      <td>0.072555</td>\n",
       "      <td>0.043340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.078661</td>\n",
       "      <td>0.047518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.961737</td>\n",
       "      <td>0.068772</td>\n",
       "      <td>0.040269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.948098</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.029636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.946570</td>\n",
       "      <td>0.054109</td>\n",
       "      <td>0.030363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.4</td>\n",
       "      <td>15</td>\n",
       "      <td>0.944667</td>\n",
       "      <td>0.052346</td>\n",
       "      <td>0.029268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.927049</td>\n",
       "      <td>0.051013</td>\n",
       "      <td>0.027806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.932297</td>\n",
       "      <td>0.048622</td>\n",
       "      <td>0.026673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.915545</td>\n",
       "      <td>0.046239</td>\n",
       "      <td>0.024943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>ADASYN</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>No target</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.524898</td>\n",
       "      <td>0.018934</td>\n",
       "      <td>0.009675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>ADASYN</td>\n",
       "      <td>Decision  Tree</td>\n",
       "      <td>No target</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.843199</td>\n",
       "      <td>0.036499</td>\n",
       "      <td>0.019095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>ADASYN</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>No target</td>\n",
       "      <td>No k</td>\n",
       "      <td>0.750757</td>\n",
       "      <td>0.030072</td>\n",
       "      <td>0.015528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Balancing Method                Model target_percentage     k  Accuracy  \\\n",
       "0        Unbalanced  Logistic Regression         No target  No k  0.994204   \n",
       "1        Unbalanced        Decision Tree         No target  No k  0.994233   \n",
       "2        Unbalanced        Random Forest         No target  No k  0.994233   \n",
       "3    Under sampling  Logistic Regression               0.1  No k  0.993599   \n",
       "4    Under sampling  Logistic Regression               0.2  No k  0.970560   \n",
       "5    Under sampling  Logistic Regression               0.3  No k  0.935296   \n",
       "6    Under sampling  Logistic Regression               0.4  No k  0.867045   \n",
       "7    Under sampling  Logistic Regression               0.5  No k  0.589026   \n",
       "8    Under sampling        Decision Tree               0.1  No k  0.988409   \n",
       "9    Under sampling        Decision Tree               0.2  No k  0.970964   \n",
       "10   Under sampling        Decision Tree               0.3  No k  0.934979   \n",
       "11   Under sampling        Decision Tree               0.4  No k  0.830512   \n",
       "12   Under sampling        Decision Tree               0.5  No k  0.819065   \n",
       "13   Under sampling        Random Forest               0.1  No k  0.993397   \n",
       "14   Under sampling        Random Forest               0.2  No k  0.988985   \n",
       "15   Under sampling        Random Forest               0.3  No k  0.971887   \n",
       "16   Under sampling        Random Forest               0.4  No k  0.856838   \n",
       "17   Under sampling        Random Forest               0.5  No k  0.793345   \n",
       "18    Over sampling  Logistic Regression               0.1  No k  0.994291   \n",
       "19    Over sampling  Logistic Regression               0.2  No k  0.984718   \n",
       "20    Over sampling  Logistic Regression               0.3  No k  0.946628   \n",
       "21    Over sampling  Logistic Regression               0.4  No k  0.904616   \n",
       "22    Over sampling  Logistic Regression               0.5  No k  0.580779   \n",
       "23    Over sampling        Decision Tree               0.1  No k  0.990946   \n",
       "24    Over sampling        Decision Tree               0.2  No k  0.969926   \n",
       "25    Over sampling        Decision Tree               0.3  No k  0.942764   \n",
       "26    Over sampling        Decision Tree               0.4  No k  0.843142   \n",
       "27    Over sampling        Decision Tree               0.5  No k  0.843142   \n",
       "28    Over sampling        Random Forest               0.1  No k  0.993772   \n",
       "29    Over sampling        Random Forest               0.2  No k  0.990283   \n",
       "..              ...                  ...               ...   ...       ...   \n",
       "51            SMOTE        Decision Tree               0.2     5  0.978893   \n",
       "52            SMOTE        Decision Tree               0.2    10  0.971944   \n",
       "53            SMOTE        Decision Tree               0.2    15  0.986996   \n",
       "54            SMOTE        Decision Tree               0.3     5  0.949540   \n",
       "55            SMOTE        Decision Tree               0.3    10  0.932499   \n",
       "56            SMOTE        Decision Tree               0.3    15  0.920158   \n",
       "57            SMOTE        Decision Tree               0.4     5  0.923272   \n",
       "58            SMOTE        Decision Tree               0.4    10  0.925464   \n",
       "59            SMOTE        Decision Tree               0.4    15  0.872351   \n",
       "60            SMOTE        Decision Tree               0.5     5  0.902050   \n",
       "61            SMOTE        Decision Tree               0.5    10  0.758427   \n",
       "62            SMOTE        Decision Tree               0.5    15  0.833569   \n",
       "63            SMOTE        Random Forest               0.1     5  0.992763   \n",
       "64            SMOTE        Random Forest               0.1    10  0.993195   \n",
       "65            SMOTE        Random Forest               0.1    15  0.993945   \n",
       "66            SMOTE        Random Forest               0.2     5  0.984141   \n",
       "67            SMOTE        Random Forest               0.2    10  0.986506   \n",
       "68            SMOTE        Random Forest               0.2    15  0.985179   \n",
       "69            SMOTE        Random Forest               0.3     5  0.966091   \n",
       "70            SMOTE        Random Forest               0.3    10  0.968254   \n",
       "71            SMOTE        Random Forest               0.3    15  0.961737   \n",
       "72            SMOTE        Random Forest               0.4     5  0.948098   \n",
       "73            SMOTE        Random Forest               0.4    10  0.946570   \n",
       "74            SMOTE        Random Forest               0.4    15  0.944667   \n",
       "75            SMOTE        Random Forest               0.5     5  0.927049   \n",
       "76            SMOTE        Random Forest               0.5    10  0.932297   \n",
       "77            SMOTE        Random Forest               0.5    15  0.915545   \n",
       "78           ADASYN  Logistic Regression         No target  No k  0.524898   \n",
       "79           ADASYN       Decision  Tree         No target  No k  0.843199   \n",
       "80           ADASYN        Random Forest         No target  No k  0.750757   \n",
       "\n",
       "    F1 Score  FBeta Score  \n",
       "0   0.000000     0.000000  \n",
       "1   0.065421     0.393215  \n",
       "2   0.029126     0.303607  \n",
       "3   0.000000     0.000000  \n",
       "4   0.046685     0.028858  \n",
       "5   0.035254     0.019441  \n",
       "6   0.031913     0.016808  \n",
       "7   0.017780     0.009102  \n",
       "8   0.137339     0.119713  \n",
       "9   0.076994     0.047398  \n",
       "10  0.058455     0.032151  \n",
       "11  0.035761     0.018659  \n",
       "12  0.037429     0.019488  \n",
       "13  0.129278     0.256345  \n",
       "14  0.165939     0.146500  \n",
       "15  0.120830     0.074120  \n",
       "16  0.046110     0.024197  \n",
       "17  0.035007     0.018158  \n",
       "18  0.000000     0.000000  \n",
       "19  0.018519     0.014681  \n",
       "20  0.038442     0.021614  \n",
       "21  0.038372     0.020549  \n",
       "22  0.018762     0.009602  \n",
       "23  0.122905     0.137177  \n",
       "24  0.081057     0.049479  \n",
       "25  0.061466     0.034211  \n",
       "26  0.042254     0.022101  \n",
       "27  0.042254     0.022101  \n",
       "28  0.136000     0.318081  \n",
       "29  0.167901     0.164322  \n",
       "..       ...          ...  \n",
       "51  0.098522     0.065587  \n",
       "52  0.081209     0.050326  \n",
       "53  0.099800     0.082792  \n",
       "54  0.063169     0.035640  \n",
       "55  0.041752     0.022924  \n",
       "56  0.036870     0.019981  \n",
       "57  0.047261     0.025668  \n",
       "58  0.040816     0.022229  \n",
       "59  0.032773     0.017291  \n",
       "60  0.035217     0.018833  \n",
       "61  0.024225     0.012520  \n",
       "62  0.026644     0.013915  \n",
       "63  0.113074     0.185790  \n",
       "64  0.106061     0.208002  \n",
       "65  0.094828     0.308783  \n",
       "66  0.112903     0.083377  \n",
       "67  0.130112     0.103369  \n",
       "68  0.095070     0.073310  \n",
       "69  0.072555     0.043340  \n",
       "70  0.078661     0.047518  \n",
       "71  0.068772     0.040269  \n",
       "72  0.052632     0.029636  \n",
       "73  0.054109     0.030363  \n",
       "74  0.052346     0.029268  \n",
       "75  0.051013     0.027806  \n",
       "76  0.048622     0.026673  \n",
       "77  0.046239     0.024943  \n",
       "78  0.018934     0.009675  \n",
       "79  0.036499     0.019095  \n",
       "80  0.030072     0.015528  \n",
       "\n",
       "[81 rows x 7 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_scores = pd.DataFrame(performance_scores, columns=['Balancing Method', 'Model','target_percentage', 'k','Accuracy', 'F1 Score', 'FBeta Score'])\n",
    "performance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>FBeta Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balancing Method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADASYN</th>\n",
       "      <td>0.706285</td>\n",
       "      <td>0.028502</td>\n",
       "      <td>0.014766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Over sampling</th>\n",
       "      <td>0.908678</td>\n",
       "      <td>0.065071</td>\n",
       "      <td>0.062263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE</th>\n",
       "      <td>0.925291</td>\n",
       "      <td>0.054718</td>\n",
       "      <td>0.046842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unbalanced</th>\n",
       "      <td>0.994224</td>\n",
       "      <td>0.031516</td>\n",
       "      <td>0.232274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under sampling</th>\n",
       "      <td>0.900260</td>\n",
       "      <td>0.064985</td>\n",
       "      <td>0.055396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Accuracy  F1 Score  FBeta Score\n",
       "Balancing Method                                 \n",
       "ADASYN            0.706285  0.028502     0.014766\n",
       "Over sampling     0.908678  0.065071     0.062263\n",
       "SMOTE             0.925291  0.054718     0.046842\n",
       "Unbalanced        0.994224  0.031516     0.232274\n",
       "Under sampling    0.900260  0.064985     0.055396"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_scores.groupby('Balancing Method').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>FBeta Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balancing Method</th>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ADASYN</th>\n",
       "      <th>Decision  Tree</th>\n",
       "      <td>0.843199</td>\n",
       "      <td>0.036499</td>\n",
       "      <td>0.019095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.524898</td>\n",
       "      <td>0.018934</td>\n",
       "      <td>0.009675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.750757</td>\n",
       "      <td>0.030072</td>\n",
       "      <td>0.015528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Over sampling</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.917984</td>\n",
       "      <td>0.069987</td>\n",
       "      <td>0.053014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.882206</td>\n",
       "      <td>0.022819</td>\n",
       "      <td>0.013289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.925844</td>\n",
       "      <td>0.102406</td>\n",
       "      <td>0.120487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">SMOTE</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.927718</td>\n",
       "      <td>0.064207</td>\n",
       "      <td>0.044310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.885086</td>\n",
       "      <td>0.021481</td>\n",
       "      <td>0.012052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.963069</td>\n",
       "      <td>0.078466</td>\n",
       "      <td>0.084163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Unbalanced</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.994233</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>0.393215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.994204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.994233</td>\n",
       "      <td>0.029126</td>\n",
       "      <td>0.303607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Under sampling</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.908786</td>\n",
       "      <td>0.069196</td>\n",
       "      <td>0.047482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.871105</td>\n",
       "      <td>0.026326</td>\n",
       "      <td>0.014842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.920890</td>\n",
       "      <td>0.099433</td>\n",
       "      <td>0.103864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Accuracy  F1 Score  FBeta Score\n",
       "Balancing Method Model                                               \n",
       "ADASYN           Decision  Tree       0.843199  0.036499     0.019095\n",
       "                 Logistic Regression  0.524898  0.018934     0.009675\n",
       "                 Random Forest        0.750757  0.030072     0.015528\n",
       "Over sampling    Decision Tree        0.917984  0.069987     0.053014\n",
       "                 Logistic Regression  0.882206  0.022819     0.013289\n",
       "                 Random Forest        0.925844  0.102406     0.120487\n",
       "SMOTE            Decision Tree        0.927718  0.064207     0.044310\n",
       "                 Logistic Regression  0.885086  0.021481     0.012052\n",
       "                 Random Forest        0.963069  0.078466     0.084163\n",
       "Unbalanced       Decision Tree        0.994233  0.065421     0.393215\n",
       "                 Logistic Regression  0.994204  0.000000     0.000000\n",
       "                 Random Forest        0.994233  0.029126     0.303607\n",
       "Under sampling   Decision Tree        0.908786  0.069196     0.047482\n",
       "                 Logistic Regression  0.871105  0.026326     0.014842\n",
       "                 Random Forest        0.920890  0.099433     0.103864"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_scores.groupby(['Balancing Method', 'Model']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>FBeta Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balancing Method</th>\n",
       "      <th>target_percentage</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADASYN</th>\n",
       "      <th>No target</th>\n",
       "      <td>0.706285</td>\n",
       "      <td>0.028502</td>\n",
       "      <td>0.014766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Over sampling</th>\n",
       "      <th>0.1</th>\n",
       "      <td>0.993003</td>\n",
       "      <td>0.086302</td>\n",
       "      <td>0.151753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.981642</td>\n",
       "      <td>0.089159</td>\n",
       "      <td>0.076161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.953788</td>\n",
       "      <td>0.074217</td>\n",
       "      <td>0.043711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>0.873629</td>\n",
       "      <td>0.043023</td>\n",
       "      <td>0.022732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.741328</td>\n",
       "      <td>0.032653</td>\n",
       "      <td>0.016961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">SMOTE</th>\n",
       "      <th>0.1</th>\n",
       "      <td>0.991020</td>\n",
       "      <td>0.072090</td>\n",
       "      <td>0.108836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.979207</td>\n",
       "      <td>0.078843</td>\n",
       "      <td>0.057467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.944840</td>\n",
       "      <td>0.050263</td>\n",
       "      <td>0.028849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>0.890206</td>\n",
       "      <td>0.039488</td>\n",
       "      <td>0.021531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.821183</td>\n",
       "      <td>0.032907</td>\n",
       "      <td>0.017526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unbalanced</th>\n",
       "      <th>No target</th>\n",
       "      <td>0.994224</td>\n",
       "      <td>0.031516</td>\n",
       "      <td>0.232274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Under sampling</th>\n",
       "      <th>0.1</th>\n",
       "      <td>0.991801</td>\n",
       "      <td>0.088872</td>\n",
       "      <td>0.125353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.976837</td>\n",
       "      <td>0.096539</td>\n",
       "      <td>0.074252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.947387</td>\n",
       "      <td>0.071513</td>\n",
       "      <td>0.041904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>0.851465</td>\n",
       "      <td>0.037928</td>\n",
       "      <td>0.019888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.733812</td>\n",
       "      <td>0.030072</td>\n",
       "      <td>0.015582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Accuracy  F1 Score  FBeta Score\n",
       "Balancing Method target_percentage                                 \n",
       "ADASYN           No target          0.706285  0.028502     0.014766\n",
       "Over sampling    0.1                0.993003  0.086302     0.151753\n",
       "                 0.2                0.981642  0.089159     0.076161\n",
       "                 0.3                0.953788  0.074217     0.043711\n",
       "                 0.4                0.873629  0.043023     0.022732\n",
       "                 0.5                0.741328  0.032653     0.016961\n",
       "SMOTE            0.1                0.991020  0.072090     0.108836\n",
       "                 0.2                0.979207  0.078843     0.057467\n",
       "                 0.3                0.944840  0.050263     0.028849\n",
       "                 0.4                0.890206  0.039488     0.021531\n",
       "                 0.5                0.821183  0.032907     0.017526\n",
       "Unbalanced       No target          0.994224  0.031516     0.232274\n",
       "Under sampling   0.1                0.991801  0.088872     0.125353\n",
       "                 0.2                0.976837  0.096539     0.074252\n",
       "                 0.3                0.947387  0.071513     0.041904\n",
       "                 0.4                0.851465  0.037928     0.019888\n",
       "                 0.5                0.733812  0.030072     0.015582"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_scores.groupby(['Balancing Method', 'target_percentage']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>FBeta Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balancing Method</th>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADASYN</th>\n",
       "      <th>No k</th>\n",
       "      <td>0.706285</td>\n",
       "      <td>0.028502</td>\n",
       "      <td>0.014766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Over sampling</th>\n",
       "      <th>No k</th>\n",
       "      <td>0.908678</td>\n",
       "      <td>0.065071</td>\n",
       "      <td>0.062263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">SMOTE</th>\n",
       "      <th>5</th>\n",
       "      <td>0.932359</td>\n",
       "      <td>0.058813</td>\n",
       "      <td>0.044849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.923908</td>\n",
       "      <td>0.054130</td>\n",
       "      <td>0.044368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.919606</td>\n",
       "      <td>0.051211</td>\n",
       "      <td>0.051308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unbalanced</th>\n",
       "      <th>No k</th>\n",
       "      <td>0.994224</td>\n",
       "      <td>0.031516</td>\n",
       "      <td>0.232274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under sampling</th>\n",
       "      <th>No k</th>\n",
       "      <td>0.900260</td>\n",
       "      <td>0.064985</td>\n",
       "      <td>0.055396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Accuracy  F1 Score  FBeta Score\n",
       "Balancing Method k                                    \n",
       "ADASYN           No k  0.706285  0.028502     0.014766\n",
       "Over sampling    No k  0.908678  0.065071     0.062263\n",
       "SMOTE            5     0.932359  0.058813     0.044849\n",
       "                 10    0.923908  0.054130     0.044368\n",
       "                 15    0.919606  0.051211     0.051308\n",
       "Unbalanced       No k  0.994224  0.031516     0.232274\n",
       "Under sampling   No k  0.900260  0.064985     0.055396"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_scores.groupby(['Balancing Method', 'k']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, se observa que el método ADASYN es el que genera menor Accuracy. Mientras que utilizar modelos sin balanceo de datos genera un Accuracy muy elevado. Lo anterior no implica que sea mejor método pues como sucede en el caso de la regresión logística, el F1 y el FB score dan muestra que ningún dato se está clasificando como positivo, llevando a conclusiones erradas. De hecho, se observa que al utilizar métodos de balanceo en los datos, el modelo con menor precisión es la regresión logística, mientras que Random Forest presenta mejor desempeño en estos casos. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
