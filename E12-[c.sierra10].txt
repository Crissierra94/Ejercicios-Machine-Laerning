Diferencias entre Gradient boosting classifier y XGB classifier
El Gradient boosting classifier es una técnica de aprendizaje automático de clasificación que aborda el problema como un problema de optimización, en el que se busca optimizar la función de pérdida. De esta manera, contiene tres elementos claves: una función de pérdida que debe ser optimizada, un weak learner que debe hacer las predicciones y un modelo iterativo que agrega weak learners con el fin de minimizar la función de pérdida. En particular, en este algoritmo, los weak learners son los árboles de decisión.
Por su parte, el XGBoost es utilizado para problemas de aprendizaje supervisado, en el que se utiliza un set de datos de entrenamiento para predecir una variable objetivo, el XGB classifier es una implementación específica del gradient boosting, en este caso los árboles pueden tener un número de nodos terminales variable. En este algoritmo el tamaño de los árboles y la magnitud de los pesos son controlados por parámetros de regularización estándar, lo que hace mucho más fuerte y robusto el modelo. El XGBoost utiliza aproximaciones más precisas para encontrar el mejor modelo de árbol de decisión. Emplea una serie de ingeniosos trucos que lo hacen exitoso, particularmente con datos estructurados.
En el GB el gradiente es básicamente la derivada parcial de la función de pérdida, por lo que describe la inclinación de la función de error. 
Una ventaja importante del XGBoost es que utiliza las derivadas de segundo orden como una aproximación para minimizar el error del modelo, lo cual proporciona mejor información sobre la dirección de los gradientes y cómo llegar al mínimo de la función de pérdida. Mientras que GB utiliza la función de pérdida del modelo base. También, utiliza una regularización avanzada lo que mejora la generalización del modelo. 
Además, tiene las ventajas que el entrenamiento puede ser muy rápido y puede ser paralelizado, es decir, distribuido en grupos.  
El XGBoost aunque implementa pocos trucos de regularización, genera una aceleración que hace valioso el algoritmo pues permite la búsqueda de hiperparámetros de forma rápida, lo cual es muy útil porque hay muchos hiperparámetros por evaluar. Casi todos ellos tienen el objetivo de limitar el sobreajuste del modelo. 
